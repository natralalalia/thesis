\chapter{Conclusion and Future Work}
\label{cha:conc}
Neuromorphic engineering has been developed for large-scale, energy-efficient, simulations on networks comprising \DIFdelbegin \DIFdel{of }\DIFdelend biologically-plausible spiking neurons.
%Nevertheless, these brain-like machines can do little and are far from being intelligent as the brain.
Nevertheless the Spiking Neural Network (SNN) has not achieved the cognitive capability and learning ability of its non-spiking counterpart, the Artificial Neural Network (ANN).
%Nevertheless, the intrinsic energy efficiency of the SNN system continues to draw attention towards the field of neuromorphic engineering.
\DIFdelbegin \DIFdel{Since deep learning techniques has }\DIFdelend \DIFaddbegin \DIFadd{Deep Learning techniques have }\DIFaddend driven simple rate-based artificial neurons to \DIFdelbegin \DIFdel{perform surpassing }\DIFdelend \DIFaddbegin \DIFadd{surpass }\DIFaddend human-level capabilities in cognitive tasks which \DIFdelbegin \DIFdel{human }\DIFdelend \DIFaddbegin \DIFadd{humans }\DIFaddend used to dominate only. 
\DIFdelbegin \DIFdel{Thus, }\DIFdelend Chapter~\ref{cha:bkg} \DIFdelbegin \DIFdel{has }\DIFdelend revealed the special features of spiking neurons that differ from \DIFaddbegin \DIFadd{the }\DIFaddend neurons of conventional ANNs.
%the core reason of the difference in cognitive abilities that the time-driven neural dynamics of spiking neuron models intrinsically differ from the abstracted rate-driven artificial neurons.
The fundamental difference between a biologically-plausible spiking neuron and an abstract artificial one raises the research problem: how to operate and train networks of biologically-plausible spiking neurons to close the gap of cognitive capabilities between SNNs and ANNs in AI tasks?

Embedding \DIFdelbegin \DIFdel{deep learning }\DIFdelend \DIFaddbegin \DIFadd{Deep Learning }\DIFaddend mechanisms for training SNNs may provide an answer to the problem.
In Chapter~\ref{cha:dnn}, the most popular and \DIFdelbegin \DIFdel{influencing deep learning }\DIFdelend \DIFaddbegin \DIFadd{influential Deep Learning }\DIFaddend models and mechanisms \DIFdelbegin \DIFdel{have been }\DIFdelend \DIFaddbegin \DIFadd{were }\DIFaddend introduced, and three of them \DIFdelbegin \DIFdel{have been }\DIFdelend \DIFaddbegin \DIFadd{were }\DIFaddend demonstrated in detail and later successfully applied on SNNs.
Most significantly, in Chapter~\ref{cha:Conv} and Chapter~\ref{cha:sdlm} we answered the main research problem of how to operate and train biologically-plausible SNNs to close the gap of cognitive capabilities between SNNs and ANNs by proposing \DIFdelbegin \DIFdel{SNN training methods }\DIFdelend both off-line and on-line \DIFaddbegin \DIFadd{SNN training methods}\DIFaddend .
Chapter~\ref{cha:bench} \DIFdelbegin \DIFdel{has }\DIFdelend provided the neuromorphic community with a uniform dataset to evaluate SNNs' performance both at model and hardware level to provide meaningful comparisons between \DIFdelbegin \DIFdel{theses }\DIFdelend \DIFaddbegin \DIFadd{these }\DIFaddend proposed SNN models and other existing methods within this rapidly advancing field of NE.%, thus to answer the research problem with strong evidence comparing to ANNs.

In this chapter, we will firstly answer the research question by confirming \DIFdelbegin \DIFdel{its hypotheses left }\DIFdelend \DIFaddbegin \DIFadd{the hypotheses proposed }\DIFaddend in Chapter~\ref{cha:intro}.
\DIFdelbegin \DIFdel{It involves }\DIFdelend \DIFaddbegin \DIFadd{This involves introducing }\DIFaddend sub-topics on how we arrived at this confirmation, what are the main contributions, and how this work challenges previous research.
This will be followed by \DIFdelbegin \DIFdel{statements of }\DIFdelend \DIFaddbegin \DIFadd{a statement of the }\DIFaddend current limitations of our study, \DIFdelbegin \DIFdel{the potential methods of }\DIFdelend \DIFaddbegin \DIFadd{potential methods for }\DIFaddend tackling these limitations, and directions for further research.

%One paragraph stating what you researched and what your original contribution to the field is…then break into sections
%Be enthused by the prospect of writing your conclusion… “It’s downhill from here…!”

%The primary achievements of the thesis are the learning methods both off-line and on-line for SNNs, which close the gap of cognitive capability between SNNs and ANNs.
%The other achievements contributes to the concerns of the feasibility of neuromorphic hardware platforms and the performance evaluation.

\section{Confirming Research Hypotheses}
\paragraph{1. Deep SNNs can be successfully and simply trained off-line where the training takes place on equivalent ANNs and then \DIFdelbegin \DIFdel{transferring }\DIFdelend the trained weights \DIFaddbegin \DIFadd{transferred }\DIFaddend back to \DIFaddbegin \DIFadd{the }\DIFaddend SNNs, thus to make them as competent as \DIFaddbegin \DIFadd{the }\DIFaddend ANNs in cognitive tasks.}
This hypothesis aims to generalise a training method on conventional ANNs whose trained connections can be transferred to corresponding SNNs with close recognition performance.

In Chapter~\ref{cha:Conv} we proposed a generalised SNN training method to train an equivalent ANN and transfer the trained weights back to SNNs.
This training procedure consists of two simple stages: first, estimate parameter $p$ for \DIFdelbegin \DIFdel{parametric activation function}\DIFdelend \DIFaddbegin \DIFadd{the Parametric Activation Function}\DIFaddend ~(PAF: $y = p \times f(x)$) with the help of the activation function we proposed, Noisy Softplus~(NSP), and second, use a PAF version of \DIFaddbegin \DIFadd{the }\DIFaddend conventional activation functions for ANN training. % can be generalised to activation units other than NSP.
%The training of a SNN model is exactly the same as ANN training, and 
The trained weights can be \DIFdelbegin \DIFdel{directly used in }\DIFdelend \DIFaddbegin \DIFadd{used directly in the }\DIFaddend SNN without any further transformation.
This method requires the least computation complexity while performing most effectively among existing algorithms.
% and even more straight-forward than the other ANN offline training methods which requires an extra step of converting ANN-trained weights to SNN's.

In terms of classification/recognition accuracy, the performance of \DIFdelbegin \DIFdel{ANN-trained SNNs }\DIFdelend \DIFaddbegin \DIFadd{the off-line trained SNN models }\DIFaddend is nearly equivalent to ANNs, and the performance loss can be partially solved by fine tuning.
The best classification accuracy of 99.07\% using LIF neurons in a PyNN simulation outperforms state-of-the-art SNN models of LIF neurons and is equivalent to the best result achieved using IF neurons~\citep{diehl2015fast}.
\DIFdelbegin \DIFdel{An }\DIFdelend \DIFaddbegin \DIFadd{Moreover, an }\DIFaddend important feature of accurately modelling LIF neurons in ANNs is the acquisition of spiking neuron firing rates. These will aid deployment of SNNs in neuromorphic hardware by providing power and communication estimates, which \DIFdelbegin \DIFdel{would }\DIFdelend allow better usage or customisation of the platforms.

With the first contributions of a simple, but effective, off-line deep SNN training method and a novel activation function, Noisy Softplus, and the achievements of successful training on \DIFdelbegin \DIFdel{an }\DIFdelend \DIFaddbegin \DIFadd{a }\DIFaddend deep spiking ConvNet which has performed close to \DIFaddbegin \DIFadd{the }\DIFaddend original ANN's\DIFaddbegin \DIFadd{, }\DIFaddend even surpassing the state-of-the-art classification accuracy, we confirm the hypothesis. 

This proposed SNN training method is simpler and even more straight-forward than the other ANN-based training approach~\citep{cao2015spiking,diehl2015fast} which requires an extra step of converting \DIFdelbegin \DIFdel{ANN-trained weights to SNN's}\DIFdelend \DIFaddbegin \DIFadd{THE trained weights for use at the SNN}\DIFaddend .
In addition, the normalisation algorithm~\citep{diehl2015fast}, proposed for \DIFdelbegin \DIFdel{weights }\DIFdelend \DIFaddbegin \DIFadd{weight }\DIFaddend transformation, only works for simple integrate-and-fire neurons, and cannot be generalised for biologically-plausible neurons.
Moreover, the novel activation function, Noisy Softplus, tackles all the problems raised by the Siegert formula which \DIFaddbegin \DIFadd{has been }\DIFaddend used to model sigmoid-like spiking neurons~\citep{Jug_etal_2012} as follows: 
	\begin{itemize}
		\item \DIFdelbegin \DIFdel{accounting of }\DIFdelend \DIFaddbegin \DIFadd{NSP accounts for the }\DIFaddend time correlation of the noisy synaptic current, e.g. $\tau_{syn}$, thus better fitting the actual response firing rate of an LIF neuron. % compared to Siegert function.

%		\item easily applied to any training method, for example BP, thanks to its derivative function defined in Equation~\ref{equ:logist}.

		\item the calculation \DIFdelbegin \DIFdel{on Noisy Softplus }\DIFdelend \DIFaddbegin \DIFadd{of NSP }\DIFaddend and its derivative is no more \DIFdelbegin \DIFdel{than }\DIFdelend \DIFaddbegin \DIFadd{calculations than the }\DIFaddend Softplus function, except for \DIFdelbegin \DIFdel{doubled computation on }\DIFdelend \DIFaddbegin \DIFadd{double computation on the }\DIFaddend weighted sum of its input ($net$ and $\sigma$ in Equations~\ref{equ:mi_input}\DIFdelbegin \DIFdel{and~\ref{equ:si_input}}\DIFdelend ).
		They are \DIFdelbegin \DIFdel{yet much more simplified than Siergert function }\DIFdelend \DIFaddbegin \DIFadd{also much simpler than the Siergert function, }\DIFaddend which saves training time and energy.

		\item as one of the ReLU-liked activation functions, the output firing rate seldom exceeds the working range of \DIFdelbegin \DIFdel{a }\DIFdelend \DIFaddbegin \DIFadd{an }\DIFaddend LIF neuron, for example the firing rates were around 0-200~Hz in the ConvNet model \DIFaddbegin \DIFadd{in Chapter~\ref{cha:Conv}}\DIFaddend .

		\item the learning performance of Noisy Softplus lies between that of Softplus and ReLU, which is supposed to outperform most of the other popular activation functions.
	\end{itemize}

\paragraph{2. Unsupervised Deep Learning modules can be trained on-line on SNNs with biologically-plausible synaptic plasticity to demonstrate a learning capability as competent as ANNs.}
This hypothesis targets the formalising of a local learning rule based on synaptic plasticity for unsupervised, event-based, biologically-plausible training of deep SNNs in order to catch up \DIFdelbegin \DIFdel{the recognition performances }\DIFdelend \DIFaddbegin \DIFadd{with the recognition performance }\DIFaddend of ANNs.

In Chapter~\ref{cha:sdlm} we proposed unsupervised \DIFdelbegin \DIFdel{deep learning }\DIFdelend \DIFaddbegin \DIFadd{Deep Learning }\DIFaddend algorithms to train SNNs purely with event-based local STDP, using a Spike-based Rate Multiplication method \DIFdelbegin \DIFdel{~}\DIFdelend (SRM).
\DIFdelbegin \DIFdel{This SRMmethod successfully transfers multiplication operations to possibility of some specific firing events of }\DIFdelend \DIFaddbegin \DIFadd{The proposed Spike-based Rate Multiplication~(SRM) method represents the product of numerical values used in these unsupervised Deep Learning techniques with rate multiplication.
The SRM then transforms the rate multiplication to the number of coincident spikes emitted from }\DIFaddend a pair of rate-coded spike trains\DIFdelbegin \DIFdel{.
Moreover, these firing }\DIFdelend \DIFaddbegin \DIFadd{, and the simultaneous }\DIFaddend events can be captured \DIFdelbegin \DIFdel{with the STDP rule and thus to update the weights accordingly in an on-line, event-based, and biologically-plausible manner.
Such weight updates suit the conventional unsupervised deep learning modules, such as Autoencoders~(AE) and Restricted Boltzmann Machines~(RBM), where multiplication of the neural outputs is the main operation}\DIFdelend \DIFaddbegin \DIFadd{by the weight change of the biologically-plausible learning rule: Spike-Time-Dependant-Plasticity~(STDP)}\DIFaddend .
During the research we encountered the problem introduced by correlated spikes, and proposed solutions to decorrelate pairs of spike trains.
It is crucial to provide deep SNNs with effective on-line training algorithms, not only for building successful spike-based object recognition applications, but also for better power efficiency and scalability when training on neuromorphic hardware.

Our second contribution is the formalisation of an STDP-based unsupervised learning \DIFdelbegin \DIFdel{algorithms of }\DIFdelend \DIFaddbegin \DIFadd{algorithm for }\DIFaddend spiking AE~(SAE) and spiking RBM~(SRBM).
These training methods \DIFdelbegin \DIFdel{have realised the }\DIFdelend \DIFaddbegin \DIFadd{realised }\DIFaddend on-line, event-based, biologically-plausible learning on spiking deep architectures in an unsupervised fashion.
The promising results of equivalent or even superior classification and reconstruction capabilities of SAEs and SRBMs compared to their conventional ANN-based methods confirms the hypothesis that SNNs have competent learning ability as deep ANNs.

%DIF > This on-line training method achieves better performance than existing algorithms and approaches the same, sometimes superior performance of the equivalent non-spiking methods.
\DIFaddbegin 

\DIFaddend Thanks to the formalisation of these proposed learning algorithms with numerical analysis, the classification results (94.72\% for SAE and 94.35\% for SRBM) have outperformed other existing SAE and SRBM models.
The first on-line training algorithm proposed by~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep{neil2013online}
}%DIFAUXCMD
, }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{neil2013online}
}%DIFAUXCMD
}\DIFaddend ignored the mathematical analysis thus achieved its best classification performance only at 81.5\%.
Neftci et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep{neftci2013event}
}%DIFAUXCMD
conducted the training on }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{neftci2013event}
}%DIFAUXCMD
conducted training on a }\DIFaddend recurrent network which was more biologically plausible but required a global signal to control the direction of the synapses and resulted in a worse classification of 91.9\% even with a bigger network \DIFdelbegin \DIFdel{comparing }\DIFdelend \DIFaddbegin \DIFadd{compared }\DIFaddend to our work.


\paragraph{3. A new set of spike-based vision datasets can provide resources to support fair competition between researchers as new concerns on energy efficiency and recognition latency emerge in Neuromorphic Vision.}
This hypothesis is expected to provide a unified spiking version of \DIFdelbegin \DIFdel{common-used dataset and }\DIFdelend \DIFaddbegin \DIFadd{a commonly-used dataset and a }\DIFaddend complementary evaluation methodology to assess the performance of SNN algorithms.

%A dataset and the corresponding evaluation methodology for comparisons of Neuromorphic Vision.
%This dataset also made the comparison of SNNs with conventional recognition methods possible by using converted spike representations of the same vision databases.
%As far as we know, this was the first attempt at benchmarking neuromorphic vision recognition by providing public a spike-based dataset and evaluation metrics.
%
%
%The first version of the dataset is published as NE15-MNIST, which contains four different spike representations of the MNIST stationary hand-written digit database.
%The Poissonian subset is intended for benchmarking existing rate-based recognition methods.
%The rank-order coded subset, FoCal, encourages research into spatio-temporal algorithms on recognition applications using only small numbers of input spikes.
%Fast recognition can be verified on the DVS recorded flashing input subset, since just 30~ms of useful spike trains are recorded for each image.
%Looking forward, the continuous spike trains captured from the DVS recorded moving input can be used to test mobile neuromorphic robots.
%\citep{orchard2015convert} have presented a neuromorphic dataset using a similar approach, but the spike trains were obtained with micro-saccades.
%This dataset aims to convert static images to neuromorphic vision input, while the recordings of moving input in our paper are intended to promote position-invariant recognition.
%Therefore, the datasets complement each other.
%
%The proposed complementary evaluation methodology is essential to assess both the model-level and hardware-level performance of SNNs.
%In addition to classification accuracy, response latency and the number of synaptic events are specific evaluation metrics for spike-based processing.
%Moreover, it is important to describe an SNN model in sufficient detail to share the network design, and relevant SNN characteristics were highlighted in the paper.  
%%For a neural network model, its topology, neuron and synapse models, and training methods are major descriptions for any kind of neural networks, including SNNs.
%%While the recognition accuracy, network latency and also the biological time taken for both training and testing are specific performance measurements of a spike-based model.
%The network size of an SNN model that can be built on a hardware platform will be constrained by the scalability of the hardware.
%Neural and synaptic models are limited to the ones that are physically implemented, unless the hardware platform supports programmability.
%Any attempt to implement an on-line learning algorithm on neuromorphic hardware must be backed by synaptic plasticity support.
%Therefore running an identical SNN model on different neuromorphic hardware exposes the capabilities of such platforms.
%If the model runs smoothly on a hardware platform, it then can be used to benchmark the performance of the hardware simulator in terms of simulation time and energy usage.
%Classification accuracy (CA) is also a useful metric for hardware evaluation because of the limited precision of the membrane potential and synaptic weights.
%
%%Although spike-based algorithms have not surpassed their non-spiking counterparts in terms of recognition accuracy, they have shown great performance in response time and energy efficiency.
In Chapter~\ref{cha:bench} we presented a dataset which contains four different spike representations of the MNIST stationary hand-written digit database. % rate-based code, rank-order code, DVS recorded flashing and moving inputs. 
The Poissonian subset is intended for benchmarking existing rate-based recognition methods.
The rank-order coded subset, FoCal, encourages research into spatio-temporal algorithms \DIFdelbegin \DIFdel{on }\DIFdelend \DIFaddbegin \DIFadd{for }\DIFaddend recognition applications using only small numbers of input spikes.
Fast recognition can be verified on the DVS recorded flashing input subset, since just 30~ms of useful spike trains are recorded for each image.
Looking forward, the continuous spike trains captured from the DVS recorded moving input can be used to test mobile neuromorphic robots.
\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep{orchard2015convert}
}%DIFAUXCMD
have }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{orchard2015convert}
}%DIFAUXCMD
}\DIFaddend presented a neuromorphic dataset using a similar approach, but the spike trains were obtained with micro-saccades.
This dataset aims to convert static images \DIFdelbegin \DIFdel{to }\DIFdelend \DIFaddbegin \DIFadd{into }\DIFaddend neuromorphic vision input, while the recordings of moving input in our paper are intended to promote position-invariant recognition.
Therefore, the datasets complement each other.

The proposed complementary evaluation methodology is essential to assess both the model-level and hardware-level performance of SNNs.
In addition to classification accuracy, response latency and the number of synaptic events are specific evaluation metrics for spike-based processing.
Moreover, it is important to describe an SNN model in sufficient detail to share the network design, and relevant SNN characteristics were highlighted in the \DIFdelbegin \DIFdel{paper}\DIFdelend \DIFaddbegin \DIFadd{study of this thesis}\DIFaddend .  
The network size of an SNN model that can be built on a hardware platform will be constrained by the scalability of the hardware.
Neural and synaptic models are limited to those that are physically implemented, unless the hardware platform supports programmability.
Any attempt to implement an on-line learning algorithm on neuromorphic hardware must be backed by synaptic plasticity support.
Therefore running an identical SNN model on different neuromorphic hardware exposes the capabilities of such platforms.
If the model runs smoothly on a hardware platform, it then can be used to benchmark the performance of the hardware simulator in terms of simulation time and energy usage.
Classification accuracy (CA) is also a useful metric for hardware evaluation because of the limited precision of the membrane potential and synaptic weights.


A third contribution of the thesis \DIFdelbegin \DIFdel{provided }\DIFdelend \DIFaddbegin \DIFadd{provides }\DIFaddend the community with a dataset and its corresponding evaluation methodology for comparisons of Neuromorphic Vision.
The published NE15-MNIST dataset contains \DIFdelbegin \DIFdel{imperative }\DIFdelend spike-based representations of the popular hand-written digits database, MNIST.
Moreover, the carefully selected evaluation metrics highlight the strengths of spike-based vision tasks and the dataset design also promotes the research into rapid and low energy recognition (e.g. flashing digits).
The successful baseline test of a benchmark system has been evaluated using the Poissonian subset of the NE15-MNIST dataset, which validates the feasibility of the database and its evaluation.
There, we confirm the hypothesis \DIFdelbegin \DIFdel{of }\DIFdelend that the dataset provides resources and supports fair comparisons among SNN models and their hardware implementations.

%
%As far as we know, this has been the first attempt at benchmarking neuromorphic vision recognition by providing public a spike-based dataset and evaluation metrics.
%In accordance with the suggestions from~\citep{tan2015bench}, the evaluation metrics highlight the strengths of spike-based vision tasks and the dataset design also promotes the research into rapid and low energy recognition (e.g. flashing digits).
%A benchmark system has been evaluated using the Poissonian subset of the NE15-MNIST dataset.
%%The models were described and their performance on accuracy, network latency, simulation time and energy usage were presented.
%These example benchmarking system has demonstrated a recommended way of using the dataset, describing the SNN models and evaluating the system performance.
%%They provide a baseline for comparisons and encourage improved algorithms and models to make use of the dataset.
%The case study has provided baselines for robust comparisons between SNN models and their hardware implementations.



\section{Future Work}
\subsection{Off-line SNN Training}
The current limitation prohibiting \DIFdelbegin \DIFdel{this }\DIFdelend \DIFaddbegin \DIFadd{the }\DIFaddend off-line SNN training method from \DIFaddbegin \DIFadd{achieving }\DIFaddend wide use lies in the lack of supporting tools.
This requires the development of a supporting tool which enables SNN training in popular \DIFdelbegin \DIFdel{deep learning }\DIFdelend \DIFaddbegin \DIFadd{Deep Learning }\DIFaddend platforms e.g. Tensorflow~\citep{tensorflow2015-whitepaper}, and an other automation tool that reads platform-dependant trained weights into \DIFaddbegin \DIFadd{the }\DIFaddend PyNN~\citep{davison2008pynn} language.

\DIFdelbegin \DIFdel{An other }\DIFdelend \DIFaddbegin \DIFadd{Another }\DIFaddend issue is the parameter calibration \DIFdelbegin \DIFdel{on the scaling factor of the combined activation, thus }\DIFdelend \DIFaddbegin \DIFadd{of $p$ for PAF;
}\DIFaddend numerical analysis is considered for \DIFdelbegin \DIFdel{future work to express the factors }\DIFdelend \DIFaddbegin \DIFadd{presenting $p$ }\DIFaddend with biological parameters of a LIF neuron.

Interesting applications have started with collaborations to extend the off-line SNN training method.
Ensemble models~\citep{rokach2010ensemble} have been \DIFdelbegin \DIFdel{considered }\DIFdelend \DIFaddbegin \DIFadd{developed }\DIFaddend to train by this method and run their SNN models on SpiNNaker to take advantage of \DIFdelbegin \DIFdel{itss massive-parallel simulating }\DIFdelend \DIFaddbegin \DIFadd{its massively-parallel simulation }\DIFaddend ability.
Speech recognition of cochlea generated spikes has achieved a promising accuracy at the initial test-idea stage.
A further goal is to implement deep networks fit for ImageNet~\citep{deng2009imagenet} tasks, which will also \DIFdelbegin \DIFdel{requires }\DIFdelend \DIFaddbegin \DIFadd{require }\DIFaddend modelling various structures of \DIFdelbegin \DIFdel{deep learning}\DIFdelend \DIFaddbegin \DIFadd{Deep Learning}\DIFaddend , for example recurrent neural networks.


\subsection{On-line \DIFdelbegin \DIFdel{biologically-plausible }\DIFdelend \DIFaddbegin \DIFadd{Biologically-Plausible }\DIFaddend Learning}
Although rate coding has shown great potential for on-line STDP learning, time-based coding carrying more information is expected to have a better or faster learning capability.
We have proposed a similar, precise-timing based learning algorithm, which although still in the test-idea stage, the prototype has shown much faster learning speed than the rate-coding mechanism.

%TODO reword this line
The modulation of STDP by a third factor such as dopamine has potentially interesting functional consequences that turn STDP from unsupervised learning into a reward-based learning paradigm~\citep{izhikevich2007solving} \DIFdelbegin \DIFdel{.
It consists to  the hot research field of reinforcement learning}\DIFdelend \DIFaddbegin \DIFadd{which address to  Reinforcement Learning}\DIFaddend .
Merging advanced neuroscience findings and \DIFdelbegin \DIFdel{deep learning mechanisms to SNN }\DIFdelend \DIFaddbegin \DIFadd{Deep Learning mechanisms onto SNNs }\DIFaddend will be the trend for future work.


Synaptic Sampling Machines (S2Ms)~\citep{neftci2016stochastic} employing a dropout~\citep{srivastava2014dropout} mechanism \DIFaddbegin \DIFadd{which }\DIFaddend hugely improved the performance of its original model from 91.9\% to 95.6\%.
Thus applying novel \DIFdelbegin \DIFdel{deep learning }\DIFdelend \DIFaddbegin \DIFadd{Deep Learning }\DIFaddend techniques for SNN training is in the future work.


\subsection{Evaluation on Neuromorphic Vision}
The database proposed in Chapter~\ref{cha:bench} will be expanded by converting more popular vision datasets to spike representations.
As mentioned in Section~\ref{sec:chapt6_intro}, face recognition has become a hot topic in SNN approaches, however there is no unified spike-based dataset to benchmark these networks.
Thus, the next development step for our dataset is to include face recognition databases.
While viewing an image, saccades direct high-acuity visual analysis to a particular object or a region of interest and useful information is gathered during the fixation of several saccades in a second.
It is possible to measure the scan path or trajectory of the eyeball and those trajectories show particular interest in eyes, nose and mouth while viewing a human face~\citep{yarbus1967eye}.
Therefore, our plan is also to embed modulated trajectory information to direct the recording using DVS sensors to simulate human saccades.

There will be more methods and algorithms for converting images to spikes.
Although Poisson spikes are the most commonly used external input to an SNN system, there are several \textit{in-vivo} recordings in different cortical areas showing that the inter-spike intervals (ISI) are not Poissonian~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep{deger2012statistical}
}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{deger2012statistical}
}%DIFAUXCMD
}\DIFaddend . 
Thus~\citep{deger2012statistical} proposed new algorithms to generate superposition spike trains of Poisson processes with dead-time (PPD) and \DIFdelbegin \DIFdel{of }\DIFdelend Gamma processes.
Including novel spike generation algorithms in the dataset is one aspect of future work which will be carried out.

%While the major stumbling crux of the computer object recognition systems lies in the invariance problem.
Each encounter of an object on the retina is unique, because of the illumination (lighting condition), position (projection location on the retina), scale (distance and size), pose (viewing angle), and clutter (visual context) variabilities.
The brain, however, recognises a huge number of objects rapidly and effortlessly even in cluttered and natural scenes.
To explore invariant object recognition, the dataset will include the NORB (NYU Object Recognition Benchmark) dataset~\citep{lecun2004learning}, which contains images of objects that are first photographed in ideal conditions and then moved and placed in front of natural scene images.

Action recognition will be the first problem of video processing to be introduced in the dataset.
The initial plan is to use the DVS retina to convert the KTH and Weizmann benchmarks to spiking versions.
Meanwhile, providing a software DVS retina simulator to transform frames into spike trains is also on the schedule.
By doing this, a huge number of videos, such as those in YouTube, can \DIFdelbegin \DIFdel{automatically be converted }\DIFdelend \DIFaddbegin \DIFadd{be converted automatically }\DIFaddend into spikes, therefore providing researchers with more time to work on their own applications.

Overall, it is impossible for the dataset proposers to provide enough datasets, converting methods and benchmarking results, thus we encourage other researchers to contribute to the dataset allowing future comparisons using the same data source.
They can also share their spike conversion algorithms by generating datasets to promote the corresponding recognition methods.
Neuromorphic hardware owners are welcome to provide benchmarking results to compare their system's performance.
\section{Summary}
%conclusion of the concolusion
%Be enthused by the prospect of writing your conclusion… “It’s downhill from here…!”
%what are the all contributions, and the significance
%what are the furture work-optional

The concluding chapter reflects the research question and hypotheses raised in the first Chapter, highlights the main contributions of the work, discusses the work in the context of literature, and propose potential methods to tackle existing limitations and promising directions for future work.
This thesis has achieved closure of the gap between the cognition capability of \DIFdelbegin \DIFdel{SNNs' to ANNs'}\DIFdelend \DIFaddbegin \DIFadd{SNN to ANN}\DIFaddend , and brightly paved the way for further study to improve and understand \DIFaddbegin \DIFadd{the }\DIFaddend learning performance of these biologically-plausible spiking units.