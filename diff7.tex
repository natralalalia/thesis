\chapter{Conclusion and Future Work}
\label{cha:conc}
Neuromorphic engineering has been developed for large-scale, energy-efficient, simulations on networks comprising \protect\TLSdel{of} biologically-plausible spiking neurons.
Nevertheless the Spiking Neural Network (SNN) has not achieved the cognitive capability and learning ability of its non-spiking counterpart, the Artificial Neural Network (ANN).
\protect\TLSdel{Since deep learning}
\protect\TLSins{Deep Learning} techniques \protect\TLSdel{has} \protect\TLSins{have} driven simple rate-based artificial neurons to \protect\TLSdel{perform surpassing} \protect\TLSins{surpass} human-level capabilities in cognitive tasks which \protect\TLSdel{human} \protect\TLSins{humans} used to dominate only. 
\protect\TLSdel{Thus,} 
Chapter~\ref{cha:bkg} \protect\TLSdel{has} revealed the special features of spiking neurons that differ from \protect\TLSins{the} neurons of conventional ANNs.
The fundamental difference between a biologically-plausible spiking neuron and an abstract artificial one raises the research problem: how to operate and train networks of biologically-plausible spiking neurons to close the gap of cognitive capabilities between SNNs and ANNs in AI tasks?

Embedding \protect\TLSdel{deep learning} \protect\TLSins{Deep Learning} mechanisms for training SNNs may provide an answer to the problem.
In Chapter~\ref{cha:dnn}, the most popular and \protect\TLSdel{influencing deep learning} \protect\TLSins{influential Deep Learning} models and mechanisms \protect\TLSdel{have been} \protect\TLSins{were} introduced, and three of them \protect\TLSdel{have been} \protect\TLSins{were} demonstrated in detail and later successfully applied on SNNs.
Most significantly, in Chapter~\ref{cha:Conv} and Chapter~\ref{cha:sdlm} we answered the main research problem of how to operate and train biologically-plausible SNNs to close the gap of cognitive capabilities between SNNs and ANNs by proposing \protect\TLSdel{SNN training methods} both off-line and \protect\TLSdel{on-line.} \protect\TLSins{on-line SNN training methods.}
Chapter~\ref{cha:bench} \protect\TLSdel{has} provided the neuromorphic community with a uniform dataset to evaluate SNNs' performance both at model and hardware level to provide meaningful comparisons between \protect\TLSdel{theses} \protect\TLSins{these} proposed SNN models and other existing methods within this rapidly advancing field of NE.
In this chapter, we will firstly answer the research question by confirming \protect\TLSdel{its} \protect\TLSins{the} hypotheses \protect\TLSdel{left} \protect\TLSins{proposed} in Chapter~\ref{cha:intro}.
\protect\TLSdel{It}
\protect\TLSins{This} involves \protect\TLSins{introducing} sub-topics on how we arrived at this confirmation, what are the main contributions, and how this work challenges previous research.
This will be followed by \protect\TLSdel{statements} \protect\TLSins{a statement} of \protect\TLSins{the} current limitations of our study, \protect\TLSdel{the} potential methods \protect\TLSdel{of} \protect\TLSins{for} tackling these limitations, and directions for further research.



\section{Confirming Research Hypotheses}
\paragraph[1. Deep SNNs can be successfully and simply trained off-line where the training takes place on equivalent ANNs and then  the trained weights transferred back to the SNNs, thus to make them as competent as the ANNs in cognitive tasks.]{1. Deep SNNs can be successfully and simply trained off-line where the training takes place on equivalent ANNs and then \protect\TLSdel{transferring} the trained weights \protect\TLSins{transferred} back to \protect\TLSins{the} SNNs, thus to make them as competent as \protect\TLSins{the} ANNs in cognitive tasks.}
This hypothesis aims to generalise a training method on conventional ANNs whose trained connections can be transferred to corresponding SNNs with close recognition performance.

In Chapter~\ref{cha:Conv} we proposed a generalised SNN training method to train an equivalent ANN and transfer the trained weights back to SNNs.
This training procedure consists of two simple stages: first, estimate parameter $p$ for \protect\TLSdel{parametric activation function~(PAF:} \protect\TLSins{the Parametric Activation Function~(PAF:} $y = p \times f(x)$) with the help of the activation function we proposed, Noisy Softplus~(NSP), and second, use a PAF version of \protect\TLSins{the} conventional activation functions for ANN training. The trained weights can be \protect\TLSdel{directly} used \protect\TLSins{directly} in \protect\TLSins{the} SNN without any further transformation.
This method requires the least computation complexity while performing most effectively among existing algorithms.

In terms of classification/recognition accuracy, the performance of ANN-trained SNNs is nearly equivalent to ANNs, and the performance loss can be partially solved by fine tuning.
The best classification accuracy of 99.07\% using LIF neurons in a PyNN simulation outperforms state-of-the-art SNN models of LIF neurons and is equivalent to the best result achieved using IF neurons~\citep{diehl2015fast}.
An important feature of accurately modelling LIF neurons in ANNs is the acquisition of spiking neuron firing rates. These will aid deployment of SNNs in neuromorphic hardware by providing power and communication estimates, which \protect\TLSdel{would} allow better usage or customisation of the platforms.

With the first contributions of a simple, but effective, off-line deep SNN training method and a novel activation function, Noisy Softplus, and the achievements of successful training on \protect\TLSdel{an} \protect\TLSins{a} deep spiking ConvNet which has performed close to \protect\TLSins{the} original \protect\TLSdel{ANN's} \protect\TLSins{ANN's,} even surpassing the state-of-the-art classification accuracy, we confirm the hypothesis. 

This proposed SNN training method is simpler and even more straight-forward than the other ANN-based training approach~\citep{cao2015spiking,diehl2015fast} which requires an extra step of converting ANN-trained weights \protect\TLSdel{to SNN's.} \protect\TLSins{for use at the SNN.}
In addition, the normalisation algorithm~\citep{diehl2015fast}, proposed for \protect\TLSdel{weights} \protect\TLSins{weight} transformation, only works for simple integrate-and-fire neurons, and cannot be generalised for biologically-plausible neurons.
Moreover, the novel activation function, Noisy Softplus, tackles all the problems raised by the Siegert formula which \protect\TLSins{has been} used to model sigmoid-like spiking neurons~\citep{Jug_etal_2012} as follows: 
	\begin{itemize}
		\item \protect\TLSdel{accounting of} \protect\TLSins{NSP accounts for the} time correlation of the noisy synaptic current, e.g. $\tau_{syn}$, thus better fitting the actual response firing rate of an LIF neuron. 

		\item the calculation \protect\TLSdel{on Noisy Softplus} \protect\TLSins{of NSP} and its derivative is no more \protect\TLSins{calculations} than \protect\TLSins{the} Softplus function, except for \protect\TLSdel{doubled} \protect\TLSins{double} computation on \protect\TLSins{the} weighted sum of its input ($net$ and $\sigma$ in \protect\TLSdel{Equations~\ref{equ:mi_input} and~\ref{equ:si_input}).} \protect\TLSins{Equations~\ref{equ:mi_input}).}
		They are \protect\TLSdel{yet} \protect\TLSins{also} much \protect\TLSdel{more simplified} \protect\TLSins{simpler} than \protect\TLSins{the} Siergert \protect\TLSdel{function} \protect\TLSins{function,} which saves training time and energy.
		
		\item as one of the ReLU-liked activation functions, the output firing rate seldom exceeds the working range of \protect\TLSdel{a} \protect\TLSins{an} LIF neuron, for example the firing rates were around 0-200~Hz in the ConvNet model.
		
		\item the learning performance of Noisy Softplus lies between that of Softplus and ReLU, which is supposed to outperform most of the other popular activation functions.
	\end{itemize}

\paragraph{2. Unsupervised Deep Learning modules can be trained on-line on SNNs with biologically-plausible synaptic plasticity to demonstrate a learning capability as competent as ANNs.}
This hypothesis targets the formalising of a local learning rule based on synaptic plasticity for unsupervised, event-based, biologically-plausible training of deep SNNs in order to catch up \protect\TLSins{with} the recognition \protect\TLSdel{performances} \protect\TLSins{performance} of ANNs.

In Chapter~\ref{cha:sdlm} we proposed unsupervised \protect\TLSdel{deep learning} \protect\TLSins{Deep Learning} algorithms to train SNNs purely with event-based local STDP, using a Spike-based Rate Multiplication \protect\TLSdel{method~(SRM).
This SRM} method \protect\TLSdel{successfully transfers} \protect\TLSins{(SRM).
represents the product of numerical values used in these unsupervised Deep Learning techniques with rate multiplication.
The SRM then transforms the rate} multiplication \protect\TLSdel{operations} to \protect\TLSdel{possibility of some specific firing events} \protect\TLSins{the number} of \protect\TLSins{coincident spikes emitted from} a pair of rate-coded spike \protect\TLSdel{trains.
Moreover, these firing} \protect\TLSins{trains, and the simultaneous} events can be \protect\TLSdel{captured with the STDP rule and thus to update} \protect\TLSins{obtained by} the \protect\TLSdel{weights accordingly in an on-line, event-based, and} biologically-plausible \protect\TLSdel{manner.
Such weight updates suit the conventional unsupervised deep} learning \protect\TLSdel{modules, such as Autoencoders~(AE) and Restricted Boltzmann Machines~(RBM), where multiplication of the neural outputs is the main operation.} \protect\TLSins{rule: Spike-Time-Dependant-Plasticity~(STDP).}
During the research we encountered the problem introduced by correlated spikes, and proposed solutions to decorrelate pairs of spike trains.
It is crucial to provide deep SNNs with effective on-line training algorithms, not only for building successful spike-based object recognition applications, but also for better power efficiency and scalability when training on neuromorphic hardware.

Our second contribution is the formalisation of an STDP-based unsupervised learning \protect\TLSdel{algorithms of} \protect\TLSins{algorithm for} spiking AE~(SAE) and spiking RBM~(SRBM).
These training methods \protect\TLSdel{have} realised \protect\TLSdel{the} on-line, event-based, biologically-plausible learning on spiking deep architectures in an unsupervised fashion.
The promising results of equivalent or even superior classification and reconstruction capabilities of SAEs and SRBMs compared to their conventional ANN-based methods confirms the hypothesis that SNNs have competent learning ability as deep ANNs.


Thanks to the formalisation of these proposed learning algorithms with numerical analysis, the classification results (94.72\% for SAE and 94.35\% for SRBM) have outperformed other existing SAE and SRBM models.
The first on-line training algorithm proposed \protect\TLSdel{by~,} \protect\TLSins{by~}\citet{neil2013online} ignored the mathematical analysis thus achieved its best classification performance only at 81.5\%.
Neftci et \protect\TLSdel{al.~} \protect\TLSins{al.~}\citet{neftci2013event} conducted \protect\TLSdel{the} training on \protect\TLSins{a} recurrent network which was more biologically plausible but required a global signal to control the direction of the synapses and resulted in a worse classification of 91.9\% even with a bigger network \protect\TLSdel{comparing} \protect\TLSins{compared} to our work.


\paragraph{3. A new set of spike-based vision datasets can provide resources to support fair competition between researchers as new concerns on energy efficiency and recognition latency emerge in Neuromorphic Vision.}
This hypothesis is expected to provide a unified spiking version of \protect\TLSdel{common-used} \protect\TLSins{a commonly-used} dataset and \protect\TLSins{a} complementary evaluation methodology to assess the performance of SNN algorithms.

In Chapter~\ref{cha:bench} we presented a dataset which contains four different spike representations of the MNIST stationary hand-written digit database. The Poissonian subset is intended for benchmarking existing rate-based recognition methods.
The rank-order coded subset, FoCal, encourages research into spatio-temporal algorithms \protect\TLSdel{on} \protect\TLSins{for} recognition applications using only small numbers of input spikes.
Fast recognition can be verified on the DVS recorded flashing input subset, since just 30~ms of useful spike trains are recorded for each image.
Looking forward, the continuous spike trains captured from the DVS recorded moving input can be used to test mobile neuromorphic robots.
\protect\TLSdel{ have}
\citet{orchard2015convert} presented a neuromorphic dataset using a similar approach, but the spike trains were obtained with micro-saccades.
This dataset aims to convert static images \protect\TLSdel{to} \protect\TLSins{into} neuromorphic vision input, while the recordings of moving input in our paper are intended to promote position-invariant recognition.
Therefore, the datasets complement each other.

The proposed complementary evaluation methodology is essential to assess both the model-level and hardware-level performance of SNNs.
In addition to classification accuracy, response latency and the number of synaptic events are specific evaluation metrics for spike-based processing.
Moreover, it is important to describe an SNN model in sufficient detail to share the network design, and relevant SNN characteristics were highlighted in the \protect\TLSdel{paper.} \protect\TLSins{study of this thesis.}  
The network size of an SNN model that can be built on a hardware platform will be constrained by the scalability of the hardware.
Neural and synaptic models are limited to those that are physically implemented, unless the hardware platform supports programmability.
Any attempt to implement an on-line learning algorithm on neuromorphic hardware must be backed by synaptic plasticity support.
Therefore running an identical SNN model on different neuromorphic hardware exposes the capabilities of such platforms.
If the model runs smoothly on a hardware platform, it then can be used to benchmark the performance of the hardware simulator in terms of simulation time and energy usage.
Classification accuracy (CA) is also a useful metric for hardware evaluation because of the limited precision of the membrane potential and synaptic weights.


A third contribution of the thesis \protect\TLSdel{provided} \protect\TLSins{provides} the community with a dataset and its corresponding evaluation methodology for comparisons of Neuromorphic Vision.
The published NE15-MNIST dataset contains \protect\TLSdel{imperative} spike-based representations of the popular hand-written digits database, MNIST.
Moreover, the carefully selected evaluation metrics highlight the strengths of spike-based vision tasks and the dataset design also promotes the research into rapid and low energy recognition (e.g. flashing digits).
The successful baseline test of a benchmark system has been evaluated using the Poissonian subset of the NE15-MNIST dataset, which validates the feasibility of the database and its evaluation.
There, we confirm the hypothesis \protect\TLSdel{of} that the dataset provides resources and supports fair comparisons among SNN models and their hardware implementations.




\section{Future Work}
\subsection{Off-line SNN Training}
The current limitation prohibiting \protect\TLSdel{this} \protect\TLSins{the} off-line SNN training method from \protect\TLSins{achieving} wide use lies in the lack of supporting tools.
This requires the development of a supporting tool which enables SNN training in popular \protect\TLSdel{deep learning} \protect\TLSins{Deep Learning} platforms e.g. Tensorflow~\citep{tensorflow2015-whitepaper}, and an other automation tool that reads platform-dependant trained weights into \protect\TLSins{the} PyNN~\citep{davison2008pynn} language.

\protect\TLSdel{An other}
\protect\TLSins{Another} issue is the parameter calibration \protect\TLSdel{on the scaling factor} of \protect\TLSdel{the combined activation, thus} \protect\TLSins{$p$ for PAF;}
numerical analysis is considered for \protect\TLSdel{future work to express the factors} \protect\TLSins{presenting $p$} with biological parameters of a LIF neuron.

Interesting applications have started with collaborations to extend the off-line SNN training method.
Ensemble models~\citep{rokach2010ensemble} have been \protect\TLSdel{considered} \protect\TLSins{developed} to train by this method and run their SNN models on SpiNNaker to take advantage of \protect\TLSdel{itss massive-parallel simulating} \protect\TLSins{its massively-parallel simulation} ability.
Speech recognition of cochlea generated spikes has achieved a promising accuracy at the initial test-idea stage.
A further goal is to implement deep networks fit for ImageNet~\citep{deng2009imagenet} tasks, which will also \protect\TLSdel{requires} \protect\TLSins{require} modelling various structures of \protect\TLSdel{deep learning,} \protect\TLSins{Deep Learning,} for example recurrent neural networks.


\subsection[On-line  Biologically-Plausible Learning]{On-line \protect\TLSdel{biologically-plausible} \protect\TLSins{Biologically-Plausible} Learning}
Although rate coding has shown great potential for on-line STDP learning, time-based coding carrying more information is expected to have a better or faster learning capability.
We have proposed a similar, precise-timing based learning algorithm, which although still in the test-idea stage, the prototype has shown much faster learning speed than the rate-coding mechanism.

The modulation of STDP by a third factor such as dopamine has potentially interesting functional consequences that turn STDP from unsupervised learning into a reward-based learning \protect\TLSdel{paradigm~.
It consists} \protect\TLSins{paradigm~}\citep{izhikevich2007solving}\protect\TLSins{ which address} to \protect\TLSdel{the hot research field of reinforcement learning.}  \protect\TLSins{Reinforcement Learning.}
Merging advanced neuroscience findings and \protect\TLSdel{deep learning} \protect\TLSins{Deep Learning} mechanisms \protect\TLSdel{to SNN} \protect\TLSins{onto SNNs} will be the trend for future work.


Synaptic Sampling Machines (S2Ms)~\citep{neftci2016stochastic} employing a dropout~\citep{srivastava2014dropout} mechanism \protect\TLSins{which} hugely improved the performance of its original model from 91.9\% to 95.6\%.
Thus applying novel \protect\TLSdel{deep learning} \protect\TLSins{Deep Learning} techniques for SNN training is in the future work.


\subsection{Evaluation on Neuromorphic Vision}
The database proposed in Chapter~\ref{cha:bench} will be expanded by converting more popular vision datasets to spike representations.
As mentioned in Section~\ref{sec:chapt6_intro}, face recognition has become a hot topic in SNN approaches, however there is no unified spike-based dataset to benchmark these networks.
Thus, the next development step for our dataset is to include face recognition databases.
While viewing an image, saccades direct high-acuity visual analysis to a particular object or a region of interest and useful information is gathered during the fixation of several saccades in a second.
It is possible to measure the scan path or trajectory of the eyeball and those trajectories show particular interest in eyes, nose and mouth while viewing a human face~\citep{yarbus1967eye}.
Therefore, our plan is also to embed modulated trajectory information to direct the recording using DVS sensors to simulate human saccades.

There will be more methods and algorithms for converting images to spikes.
Although Poisson spikes are the most commonly used external input to an SNN system, there are several \textit{in-vivo} recordings in different cortical areas showing that the inter-spike intervals (ISI) are not \protect\TLSdel{Poissonian~.} \protect\TLSins{Poissonian~}\citet{deger2012statistical}\protect\TLSins{.} 
Thus~\citep{deger2012statistical} proposed new algorithms to generate superposition spike trains of Poisson processes with dead-time (PPD) and \protect\TLSdel{of} Gamma processes.
Including novel spike generation algorithms in the dataset is one aspect of future work which will be carried out.

Each encounter of an object on the retina is unique, because of the illumination (lighting condition), position (projection location on the retina), scale (distance and size), pose (viewing angle), and clutter (visual context) variabilities.
The brain, however, recognises a huge number of objects rapidly and effortlessly even in cluttered and natural scenes.
To explore invariant object recognition, the dataset will include the NORB (NYU Object Recognition Benchmark) dataset~\citep{lecun2004learning}, which contains images of objects that are first photographed in ideal conditions and then moved and placed in front of natural scene images.

Action recognition will be the first problem of video processing to be introduced in the dataset.
The initial plan is to use the DVS retina to convert the KTH and Weizmann benchmarks to spiking versions.
Meanwhile, providing a software DVS retina simulator to transform frames into spike trains is also on the schedule.
By doing this, a huge number of videos, such as those in YouTube, can \protect\TLSdel{automatically} be converted \protect\TLSins{automatically} into spikes, therefore providing researchers with more time to work on their own applications.

Overall, it is impossible for the dataset proposers to provide enough datasets, converting methods and benchmarking results, thus we encourage other researchers to contribute to the dataset allowing future comparisons using the same data source.
They can also share their spike conversion algorithms by generating datasets to promote the corresponding recognition methods.
Neuromorphic hardware owners are welcome to provide benchmarking results to compare their system's performance.
\section{Summary}

The concluding chapter reflects the research question and hypotheses raised in the first Chapter, highlights the main contributions of the work, discusses the work in the context of literature, and propose potential methods to tackle existing limitations and promising directions for future work.
This thesis has achieved closure of the gap between the cognition capability of \protect\TLSdel{SNNs'} \protect\TLSins{SNN} to \protect\TLSdel{ANNs',} \protect\TLSins{ANN,} and brightly paved the way for further study to improve and understand \protect\TLSins{the} learning performance of these biologically-plausible spiking units.