\chapter{Noisy Softplus: an Activation Function Enables Training Deep SNNs Off-line as ANNs}
\label{cha:Conv}
%Paragraph One: LINK
The first step in answering my research question of validating the capability of real-time cognitive application built on neuromorphic platform was to build an object recognition prototype running completely on hardware in an absolute spike-based fashion.
It built up the basis for the later research that neuromorphic hardware has been capable of running large cognitive applications standing alone without a general computer and has stepped into the era of improving its cognitive ability.
It is also possible to make a link between this chapter and the whole argument that this work contributes to an effective SNN training method which performs equivalently to ANNs.

%Paragraph Two: FOCUS
%Now focus the reader’s attention on what this chapter is specifically going to do and why it is important. In this chapter I will examine.. I will present… I will report … This is crucial in (aim of thesis/research question) in order to….
The Spiking Neural Network (SNN) has not achieved the recognition/classification performance of its non-spiking competitor, the ANN, particularly when used in deep neural networks.
Training SNNs by conventional ANN algorithms and mapping well-trained ANNs weights to SNNs are attracting attention in this field for its straight-forward mechanism.
%, especially the SNNs of spiking neurons with biological characteristics.
In this chapter we will propose a new biologically-inspired activation function, Noisy Softplus, which is well-matched to the response firing activity of LIF (Leaky Integrate-and-Fire) neurons.
Thus, any ANN employing Noisy Softplus neurons, even of deep architecture, can be trained simply by the traditional algorithm, for example Back Propagation~(BP), and the trained weights can be directly used in the spiking version of the same network without any conversion.
Furthermore, the training method can be generalised to other activation units, for instance ReLU, to train deep SNNs off-line.
This research is crucial to 1) provide an effective approach for SNN training, and 2) increase the classification accuracy of SNNs using biological characteristics and close the gap between the performance of SNNs and ANNs.

%Paragraph Three: OVERVIEW
%The third paragraph simply outlines the way that you are going to achieve the aim spelled out in the previous paragraph. It’s really just a statement of the contents in the order that the reader will encounter them. It is important to state these not simply as topics, but actually how they build up the internal chapter argument… I will begin by examining the definitions of, then move to seeing how these were applied… I first of all explain my orientation to the research process, positioning myself as a critical scholar.. I then explain the methodology that I used in the research, arguing that ethnography was the most suitable approach to provide answers to the question of… 
%https://patthomson.net/2014/01/16/connecting-chapterschapter-introductions/

%We first of all explore the research question of on-line, event-based deep SNN training in the literature.
%We then describe SRM in mathematical expression, and explain the factors influencing the accuracy of the method and the factors not. 
%Afterwards we argue why the learning algorithm is proper to train spiking Autoencoders (SAEs) and Spiking Restricted Boltzmann Machines (SRBM).
%During the research we encountered the problem introduced by the correlated spikes, therefore we also propose solutions to decorrelate spike trains in Section~\ref{sec:problem}. 
%Finally the detailed comparison of the traditional training and SRM learning on MNIST dataset demonstrates the equivalent learning ability and shows similar even surpassing recognition and reconstruction capability of SAE and SRBM.
In this chapter, we begin by introducing the difficulty of SNN training, and lead to the existing solutions state-of-the-art.
Then we put forward the proposed activation function, Noisy Softplus, and demonstrate how it fits the network dynamics composed of spiking neurons.
Afterwards, a complete SNN training method is explained employing the Noisy Softplus activations and is generalised to other activation functions.
To validate the classification accuracy, a convolutional network (ConvNet) is trained on the MNIST database following this training mechanism and tested directly on an SNN.
The encouraging result demonstrates a close recognition capability of the more biologically-realistic SNNs compared to the conventional ANNs.

\section{Introduction}	
Deep Neural Networks (DNNs) are the most promising research field in computer vision, even exceeding human-level performance on image classification tasks~\cite{he2015delving}.
To investigate whether brains might work similarly on vision tasks, these powerful DNN models have been converted to spiking neural networks (SNNs).
In addition, the spiking DNN offers the prospect of neuromorphic systems that combine remarkable performance with energy-efficient training and operation.

Theoretical studies have shown that biologically-plausible learning, e.g. Spike-Timing-Dependent Plasticity (STDP), could approximate a stochastic version of powerful machine learning algorithms
%	~\cite{nessler2013bayesian,neftci2013event,neftci2013event,o2016deep}.
such as 
%	Expectation Maximization~\cite{nessler2013bayesian}, 
Contrastive Divergence~\cite{neftci2013event}, Markov Chain Monte Carlo~\cite{buesing2011neural} and Gradient Descent~\cite{o2016deep}.
It is the stochasticity, in contrast with the continuously differentiable functions used by ANNs, that is intrinsic to the event-based spiking process, making network training difficult.
In practice, ANNs use neuron and synapse models very different from biological neurons, and it remains an unsolved problem to develop SNNs with equivalent performance.

%How to map a well trained ANN to SNN is a hot topic in this field, especially using spiking neurons of biological scale.

Conversely, the offline training of an ANN, which is then mapped to an SNN, has shown near loss-less conversion and state-of-the-art classification accuracy.
These research aim to prove that SNNs are equally capable as their non-spiking rivals of pattern recognition, and at the same time are more biologically realistic and energy-efficient.
Jug et. al.~\cite{Jug_etal_2012} first proposed the use of the Siegert function to replace the sigmoid activation function in training Restricted Boltzmann Machine (RBM).
The Siegert units map incoming currents driven by Poisson spike trains to the response firing rate of a Leaky Integrate-and-Fire (LIF) neuron.
The ratio of the spiking rate to its maximum is equivalent to the output of a sigmoid neuron.
A spiking Deep Belief Network (DBN)~\cite{Stromatias2015scalable} of four layers of RBMs was implemented on neuromorphic hardware, SpiNNaker~\cite{furber2014spinnaker}, to recognise hand written digits in real time.

However, cortical neurons seldom saturate their firing rate.
Thus Rectified Linear Units (ReLU) were proposed to replace sigmoid neurons and surpassed the performance of other popular activation units thanks to their advantage of sparsity~\cite{glorot2011deep} and its robustness towards the vanishing gradient problem. % which will be described in Section~\ref{sec:activation}.
%TODO Chapter2 about relu and vanishing gradient problem.
%	~\cite{dugas2001incorporating}.
%trained with noisy input, the 4-layered spiking autoencoder reached 98.37\% accuracy on MNIST. 
Recent development on ANN-trained SNN models has focused on using ReLU units and converting trained weights to SNNs.
Better performance~\cite{cao2015spiking,diehl2015fast} has been demonstrated in Spiking Convolutional Networks (ConvNets), but this employed simple integrate and fire (IF) neurons.
The training used only ReLUs and zero bias to avoid negative outputs, and applied a deep learning technique, dropout~\cite{srivastava2014dropout}, to increase the classification accuracy.
Normalising the trained weights for use on an SNN was relatively straightforward and maintained the classification accuracy.
This work was extended to a Recursive Neural Network (RNN)~\cite{diehl2016conversion} and run on the TrueNorth~\cite{merolla2014million} neuromorphic hardware platform.
Except for the popular, simplified version of ReLU, $max(0,\sum w x)$, the other implementation of $\log(1+e_x)$, Softplus, is more biologically realistic.
Recent work~\cite{hunsberger2015spiking} proposed the Soft LIF response function for training SNNs, which is equivalent to Softplus activation of ANNs.
	
\section{Modelling The Activation Function}
We have introduced the existing work of modelling the response firing activity of LIF neurons using Siegert~\cite{Jug_etal_2012} function.
It created the idea of modelling activation functions for spiking neurons whose input is synaptic current generated by spike arrivals and output is the firing rate of a sequence of spike train.
Although the use of Siegert function opened the door for ANN-trained SNNs, there are several drawbacks of this method:
\begin{itemize}
	\item most importantly, the numerical analysis on the LIF response function is too far from the practice, thus it generates large error between the model and the actual response firing rate.
	This issue will be addressed in Section~\ref{subsec:practice}.
	\item the training uses Siegert function to estimate the output firing rate in the forward path, but uses the derivative of sigmoid function during back propagation.
	Due to the difference of Siegert and sigmoid functions, error is introduced. 
	\item high complexity of Siegert function causes much longer training time and more energy.
	\item neurons have to fire at high frequency (higher than half of the maximum firing rate) so as to represent activation of a sigmoid unit, as a result the network activity requires high power dissipation.
	\item better learning performance has been reported using ReLU, so modelling ReLU like activation function of Spiking neurons is needed.  
\end{itemize}

This section will propose the Noisy Softplus function which provides solutions to the drawbacks of Siegert unit.
\subsection{Neural Science Background}
	%As discussed above, most of the top scored networks map the ReLUs in ANN to the equivalent spiking version of IF neurons.
	%This research proposes a new activation function, Noisy Softplus, which is inspired by neuroscience observations of LIF neurons.
	The LIF neuron model follows the following membrane potential dynamics:
	\begin{equation}
	\tau_m \frac{\D V}{\D t}=V_{rest} - V + R_{m} I(t) ~.
	\label{equ:LIF_V}
	\end{equation}
	The membrane potential $V$ changes in response to the input current $I$, starting at the resting membrane potential $V_{rest}$, where the membrane time constant is $\tau_m = R_mC_m$, $R_m$ is the membrane resistance and $C_m$ is the membrane capacitance.
	The central idea in converting spiking neurons to activation units lies in the response function of a neuron model.
	Given a constant current injection $I$, the response function, i.e. firing rate, of the LIF neuron is:
	\begin{equation}
	\lambda_\mathit{out}=
	\left [ t_\mathit{ref}-\tau_m\log \left ( 1-\frac{V_{th}-V_\mathit{rest}}{IR_m}  \right )\right ]^{-1}, \textrm{~when~} IR_m>V_{th}-V_{rest},
	\label{equ:consI}
	\end{equation}
	otherwise the membrane potential cannot reach the threshold $V_{th}$ and the output firing rate is zero. 
	The absolute refractory period $t_\mathit{ref}$ is included, where all input during this period is ignored.
	
	%TODO very important part
	However, in practice, a noisy current generated by the random arrival of spikes, rather than a constant current, flows into the neurons.
	%White noise
	The noisy current is typically treated as a sum of deterministic constant term, $I_{const}$, and a white noise term, $I_{noise}$.
	Thus the value of the current is Gaussian distributed with $m_I$ mean and ${s_I}^2$ variance.
	The white noise is a stochastic process $\xi(t)$ with mean 0 and variance 1, which is delta-correlated, i.e., the process is uncorrelated in time that a value $\xi(t)$ at time $t$ is totally independent of the value at any other time $t'$.
	Therefore, the noisy current can be seen as:
	\begin{equation}
	I(t) = I_{const}(t)+I_{noise}(t) = m_I + s_I\xi(t)~~,
	\label{equ:noisyI}
	\end{equation}
	and accordingly, Equation~\ref{equ:LIF_V} becomes:
	\begin{equation}
	\frac{\D V}{\D t}=\frac{V_{rest} - V}{\tau_m } + \frac{m_I}{C_m} + \frac{s_I\xi(t)}{C_m}~~.
	\label{equ:LIF_V2}
	\end{equation}
	
	We then multiply the both sides of Equation~\ref{equ:LIF_V2} by a short time step $\D t$, the stochastic differential equation of the membrane potential satisfies an Ornstein–Uhlenbeck process:
	\begin{equation}
	\begin{aligned}
	\D V&= \frac{V_{rest} - V}{\tau_m }\D t + \frac{m_I}{C_m} \D t + \frac{s_I}{C_m}  \D W_t \\
	&=\frac{V_{rest} - V}{\tau_m }\D t + \frac{m_I}{C_m} \D t + \frac{s_I \sqrt{\D t}}{C_m} \xi(t)  \\
	&=\frac{V_{rest} - V}{\tau_m }\D t + \mu \D t + \sigma \xi(t) ~~. 
	\end{aligned}
	\end{equation}	
	The last term $\D W_t$ is a Wiener process, that $W_{t +\D t} - W_{t}$ obeys Gaussian distribution with mean 0 and variance $\D t$.
	The instantaneous mean $\mu$ and variance $\sigma^2$ of the change in membrane potential characterise the statistics of $V$ in a short time range, and they can be derived from the statistics of the noisy current:
	\begin{equation}
	\mu =\dfrac{m_I}{C_m}, ~~~~~ \sigma = \dfrac{s_I \sqrt{\D t}}{C_m}~.
	\end{equation}
%	A continuous-time stochastic process W(t) for t>=0 with W(0)=0 and such that the increment W(t)-W(s) is Gaussian with mean 0 and variance t-s for any 0<=s<t, and increments for nonoverlapping time intervals are independent. Brownian motion (i.e., random walk with random step sizes) is the most common example of a Wiener process.
%http://mathworld.wolfram.com/WienerProcess.html
%http://neuronaldynamics.epfl.ch/online/Ch1.S3.html about V tradractory need to write in Chapter 2.	
	The response function~\cite{rauch2003neocortical,la2008response} of the LIF neuron to a noisy current, also known as Siegert formula~\cite{siegert1951first}, is driven by the $\mu$ and $\sigma$:
	\begin{equation}
	\lambda_\mathit{out}=
	\left [ t_\mathit{ref}+\tau_m \int_{\frac{V_\mathit{rest}-\mu \tau_m }{\sigma \sqrt{\tau_m}}}^{\frac{V_{th}-\mu \tau_m }{\sigma \sqrt{\tau_m}}} \sqrt{\pi} \exp(u^{2}) (1+erf(u)) \D u \right ]^{-1} ~,
	\label{equ:siegert}
	\end{equation}
	

	
	\begin{figure}[bt]
		\centering
		\includegraphics[width=0.7\textwidth]{pics_iconip/1.pdf}
		\caption{Response function of the LIF neuron with noisy input currents with different standard deviations.}
		\label{Fig:physics}
	\end{figure}
	
	Figure~\ref{Fig:physics} shows the response curves (Equation~\ref{equ:siegert}) of a LIF neuron driven by noisy currents with Gaussian noise of $\mu$ mean and $\sigma$ standard deviation.
	The parameters of the LIF neuron are all biologically valid (see the listed values in Table~\ref{tbl:pynnConfig}), and the same parameters are used throughout this chapter.
	The solid (zero noise) line in Figure~\ref{Fig:physics} illustrates the response function of such an LIF neuron injected with constant current, which inspired the proposal of ReLUs.
	As noise increases, the level of firing rates also rises, and the Softplus activation function approximates the response firing activity driven by current with Gaussian white noise adding on.
	Softplus units only represent a single level of noise that, for example, the dotted line in Figure~\ref{Fig:physics} is drawn when $\sigma=1$.
	
		\begin{table}[bt]
			\centering
			\caption{\label{tbl:pynnConfig}Parameter setting for the current-based LIF neurons using PyNN.}
			\bgroup
			\def\arraystretch{1.4}
			\begin{tabular}{c c c}
				%\hline
				Parameters & Values & Units \\
				\hline
				cm & 0.25 & nF	\\
				%\hline
				tau\_m & 20.0 & ms\\
				%\hline
				tau\_refrac & 1.0 & ms\\
				%					%\hline
				%					tau\_syn\_E & 5.0 & ms\\
				%					%  %\hline
				%					tau\_syn\_I & 5.0 & ms\\
				%  %\hline
				v\_reset & -65.0 & mV\\
				%\hline
				v\_rest & -65.0 & mV\\
				%\hline
				v\_thresh & -50.0 & mV\\
				%\hline
				i\_offset & 0.1 & nA\\
				\hline
				%			Parameters & cm & tau\_m & tau\_refrac & tau\_syn\_E & tau\_syn\_I & v\_rest & v\_thresh & i\_offset \\
				%			Values & 0.25 &  20.0 & 1.0 & 5.0 & 5.0 & -65.0 & -50.0  & 0.1 \\
				%			Units & nF & ms & ms & ms & ms & mV & mV& nA\\
				%			\hline
			\end{tabular}
			\egroup
		\end{table}
	

	


%	\subsection{Activation Function in ANN}
%	\label{sec:activation}
%	Nair and Hinton~\cite{nair2010rectified} stated that ReLU preserve information about relative intensities(intensity equivarance) provided they have zero biases and are noise-free, but binary units do not.
%	This technique has dominated the best recognition performances~\textcolor{red}{ref?} in deep learning.
%	Furthermore, utilising ReLU in SNNs will simplify the network structure significantly because of the zero biases.
%	Thus in this section we firstly demonstrate on how ReLU work and perform in DBN.
%	
%	The input vector in a ReLU based RBM does no longer have to be binary, so it can be a float.
%	Instead of using sigmoid transfer function, the ReLU is always positive and has no upper boundary.
%	There are three different ways to model ReLU~\ref{Fig:relu_tranf}:
%	\begin{figure}[hbt]
%		\centering
%		\includegraphics[width=0.7\textwidth]{pics_sdbn/relu.pdf}
%		\caption{The DBN architecture.} 
%		\label{Fig:relu_tranf}
%	\end{figure}
%	\begin{itemize}
%		\item sum of an infinite number of binary units with each having a bias less than the previous one~~\cite{nair2010rectified}.
%		This is one of the reason people believe the recognition performance exceeding since each ReLU represents infinite samples of sigmoid. 
%		\item the approximation of the previous form $\log(1+e_x)$.
%		\item the simplified version which is the most popular in deep learning $max(0,\sum w x)$.
%	\end{itemize}  
	
	
	
	
	\subsection{LIF Neuron Simulation}
	\label{subsec:practice}
	
	
	\begin{figure}[tbp!]
		\centering
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/2-1.pdf}
			\caption{1~ms resolution}
			\label{Fig:current-1}
		\end{subfigure}
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/2-10.pdf}
			\caption{10~ms resolution}
			\label{Fig:current-10}
		\end{subfigure}
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/curr_dt1.pdf}
			\caption{1~ms resolution}
			\label{Fig:signal-1}
		\end{subfigure}
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/curr_dt10.pdf}
			\caption{10~ms resolution}
			\label{Fig:signal-10}
		\end{subfigure}
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/distr_dt1.pdf}
			\caption{1~ms resolution}
			\label{Fig:gaussian-1}
		\end{subfigure}
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/distr_dt10.pdf}
			\caption{10~ms resolution}
			\label{Fig:gaussian-10}
		\end{subfigure}
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/psd_dt1.pdf}
			\caption{1~ms resolution}
			\label{Fig:spectrum-1}
		\end{subfigure}
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/psd_dt10.pdf}
			\caption{10~ms resolution}
			\label{Fig:spectrum-10}
		\end{subfigure}
		\caption{Comparing the recorded firing rates of the LIF neuron simulation driven by noisy currents to the response function shown in Figure~\ref{Fig:physics}.}
		\label{Fig:lif_curr}
	\end{figure}

	To verify the response function in practice, simulation tests were carried out using PyNN~\cite{davison2008pynn} to compare with the analytical results.
	The noisy current was produced by a constant current of $m_I$ adding up with a Gaussian white noise of zero mean and $s_I^2$ variance.
	The noise was drawn from the Gaussian distribution in a time resolution of $dt$.
	We chose $dt=1$~ms which is the finest resolution for common SNN simulators, and $dt=10$~ms for comparison.
	For a given pair of $m_I$ and $s_I^2$, a noisy current was injected into a current-based LIF neuron for 10~s, and the output firing rate was the average among 10 trials.
	
	The curves in Figures~\ref{Fig:lif_curr}(a) and (b) lined up the tests on same noise level.
	The difference compared to the analytical results (thin lines) comes from the time resolution of the simulated noisy current, $dt$.
	The discrete sampling of the noisy current introduces time correlation to the white noise, shown in  Figure~\ref{Fig:lif_curr}(d), where the current value during a time step $dt$ remains the same.
	Although both the current signals followed the same Gauss distribution, see Figures~\ref{Fig:lif_curr}(e) and (f), as a sampling frequency of 1kHz the current is a white noise when $dt=1$~ms, but a coloured noise when $dt=10$~ms, see the spectrum analysis in Figures~\ref{Fig:lif_curr}(g) and (h).
	Thus the Siegert formula, Equation~\ref{equ:siegert}, can only approximate the LIF response of noisy current with white noise.
	
\begin{figure}[btp!]
	\centering
	\begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[width=\textwidth]{pics_iconip/spiked_curve_1.pdf}
		\caption{$\tau_{syn}$=1~ms}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[width=\textwidth]{pics_iconip/spiked_curve_10.pdf}
		\caption{$\tau_{syn}$=10~ms}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[width=\textwidth]{pics_iconip/curr_tau1.pdf}
		\caption{$\tau_{syn}$=1~ms}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[width=\textwidth]{pics_iconip/curr_tau10.pdf}
		\caption{$\tau_{syn}$=10~ms}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[width=\textwidth]{pics_iconip/distr_tau1.pdf}
		\caption{$\tau_{syn}$=1~ms}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[width=\textwidth]{pics_iconip/distr_tau10.pdf}
		\caption{$\tau_{syn}$=10~ms}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[width=\textwidth]{pics_iconip/psd_tau1.pdf}
		\caption{$\tau_{syn}$=1~ms}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[width=\textwidth]{pics_iconip/psd_tau10.pdf}
		\caption{$\tau_{syn}$=10~ms}
	\end{subfigure}
	\caption{Recorded response firing rate of a LIF neuron.
		The driving noisy current is simulated with Poission spike trains and the results are compared to the noisy current source.}
	\label{Fig:lif_pois}
\end{figure}

	A more realistic simulation of a noisy current is generated by a Poisson spike train, 
	%assuming that large amount of small amplitude PSPs are required to reach the threshold and $\tau_syn$ limits to 0.
	where the mean and variance are given by:
	\begin{equation}
	m_I = \tau_{syn}\sum_i w_i\lambda_{i}~, ~s_I^2=\frac{1}{2}\tau_{syn}\sum_i w_i^2\lambda_{i}~,
	\label{equ:distr}
	\end{equation}
	where $\tau_{syn}$ is the synaptic time constant, and each Poisson spike train connects to the neuron with a strength of $w_i$ and a firing rate of $\lambda_i$.
	Two populations of Poisson spike sources, for excitatory and inhibitory synapses respectively, were connected to a single LIF neuron to mimic the noisy currents.
	The firing rates of the Poisson spike generators were determined by the given $m_I$ and $s_I$.
	Figures~\ref{Fig:lif_pois}(a) and (b) illustrate the recorded firing rates responding to the spike trains compared to the mean firing rate driven by noisy currents.
	Note that the estimation of LIF response activity requires noisy current with white noise, however
%	The use of noisy currents assumes that the post-synaptic potential (PSP) is a delta function, e.g. $\tau_{syn}$ tends to the limits of 0.
	in practice the release of neurotransmitter takes time ($\tau_{syn} >> 0$) and the synaptic current decays exponentially $I_{syn} = I_0 e^{\frac{-t}{\tau_{syn}}}$.
	Figures~\ref{Fig:lif_pois}(c) and (d) show two examples of synaptic current of mean 0~nA and standard deviation 0.2 driven by 100 neurons firing at the same rate and with the same synaptic strength, but of different synaptic time constant.
	Therefore, the current at any time t during decaying period is dependant to the value at previous time step, which makes the synaptic current a coloured noise, see Figures~\ref{Fig:lif_pois}(g) and (h).
%	and the noise added to the mean current is not white noise, but pink noise.
	We observe in Figure~\ref{Fig:lif_pois}(a) that the response firing rate to synaptic current is higher than the Gauss noisy current for all the 10 trials.
	It is caused by a) the coarse resolution (1~ms) of the spikes, thus the standard deviation of the current is larger than 0.2, shown in Figure~\ref{Fig:lif_pois}(e);
	and b) the $\tau_{syn}$, even short as 1~ms, adds coloured noise instead of white noise to the current.
	While Figure~\ref{Fig:lif_pois}(b) shows a similar firing rate of both the synaptic driven current and the Gauss noisy current, since both of the current signals have similar distribution and time correlation.
	Nevertheless, both the practical simulations cannot be approximated by analytical response function.
	
%	Thus the experiments show that a longer $\tau_{syn}$ increases the level of noise and widens the variance of the output firing rate.
	

		
	\subsection{Noisy Softplus}
	Therefore, due to the limited time resolution of common SNN simulators and the time taken for neurotransmitter release, $\tau_{syn}$, mismatches exist between the analytical response function, Siegert formula, and the practical neural activities.
	Consequently, a unified activation function is required to model the practical responses of a LIF neuron.
	Inspired by the set of response functions triggered by different levels of noise, we propose the Noisy Softplus activation function:
	\begin{equation}
	y = f_{ns}(x, \sigma) = k \sigma \log [1 + \exp(\frac{x}{k \sigma})],
	\label{equ:nsp}
	\end{equation}
	where $x$ refers to the mean current, $y$ is the normalised output firing rate, $\sigma$ plays an important role to define the noise level, and $k$, which is determined by the neuron parameters, controls the shape of the curves.
	Note that the novel activation function we propose contains two parameters, the current and its noise; both are naturally obtained in spiking neurons.
	% With doubled information, more powerful training methods and network models are expected. 
	Figure~\ref{fig:nsp} shows the activation function in curve sets corresponding to different discrete noise level, and in a 3D plot.
	\begin{figure}[thb!]
		\centering
		\begin{subfigure}[t]{0.7\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/4.pdf}
			\caption{Noisy Softplus}
		\end{subfigure}\\
		\begin{subfigure}[t]{0.8\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/5.pdf}
			\caption{Noisy Softplus in 3D}
		\end{subfigure}
		\caption{
			Noisy Softplus fits to the response function of the LIF neuron.
			Noisy Softplus in (a) curve sets and (b) 3D.}
		\label{fig:nsp}
	\end{figure}	
	
	The derivative is the logistic function scaled by $k\sigma$:
	\begin{equation}
	\frac{\partial f_{ns}(x,\sigma)}{\partial x} = \frac{1}{1+exp(-\frac{x}{k\sigma})}~~,
	\label{equ:logist}
	\end{equation}	
	which could be easily applied to back propagation in any network training.
	
	The normalised activation function can be scaled up by a factor, $S$, to represent the firing rate $\lambda$ of a LIF neuron driven by a noisy current $x$.
	\begin{equation}
	\begin{aligned}
	\lambda_{out} &\simeq f_{ns}(x, \sigma) \times S\\
	&=k \sigma \log [1 + \exp(\frac{x}{k \sigma})] \times S
	\end{aligned}
	\label{equ:fit}
	\end{equation}	
	
	The Noisy Softplus fits well to the practical response firing rate of the LIF neuron with suitable calibration of $k$ and $S$, see Figure~\ref{Fig:nsptau1}.
	The parameter pair of $(k, S)$ is curve-fitted with the triple data points of $(\lambda, x, \sigma)$.
	The fitted parameter was set to $(k, S)=(0.19,208.76)$ for the practical response firing rate driven by synaptic noisy current with $\tau_{syn}=1$~ms and was set to $(k, S)=(0.35,201.06)$ when $\tau_{syn}=10$~ms.
	The calibration currently is conducted by linear least squares regression, however numerical analysis is considered for future work to express the factors, $k$ and $S$ with the biological parameters of a LIF neuron.
	
	\begin{figure}
		\centering
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/4-1.pdf}
			\caption{$\tau_{syn}$=1~ms}
		\end{subfigure}
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/4-10.pdf}
			\caption{$\tau_{syn}$=10~ms}
		\end{subfigure}
		\caption{Noisy Softplus fits to the response firing rates of LIF neurons.}
		\label{Fig:nsptau1}
	\end{figure}		
	
	
\section{ANN-Trained SNNs}	
	We have discussed modelling response firing activity of a LIF neuron with a unified activation function, Noisy Softplus.
	However the demonstration of mapping the physical activity to numerical ANN calculations is still required, thus to train the layered-up deep network.
	
	\subsection{Equivalent Input and Output}
	
	\begin{figure}[bt]
		\centering
		\includegraphics[width=0.7\textwidth]{pics_sdlm/neuron.pdf}
		\caption{Artificial neuron model in ANNs. }
		\label{Fig:neuron}
	\end{figure}
	Neurons in ANNs take inputs from their previous layer, and feed the weighted sum of their input, $net_j = \sum_i w_{ij}x_i$, to the activation function.
	The transformed signal then forms the output of an artificial neuron, $y_j=f(net_j)$, see Figure~\ref{Fig:neuron}.
	
	Meanwhile, Noisy Softplus is able to predict the firing rate of a spiking neural driven by some noisy current, nevertheless an extra step is still needed to map the firing rate to numerical values in ANNs.
	According to Equation~\ref{equ:distr}, the mean of the current feeding into a spiking neuron is equivalent to $net$ of artificial neurons, where
	\begin{equation}
	\begin{aligned}
		& {m_I}_j = \sum_i w_{ij}(\lambda_{i}\tau_{syn})~, \textrm{  thus}\\
		& net_j= \sum_i w_{ij} x_i~~, \textrm{~~and~~}
		x_i = \lambda_{i}\tau_{syn}~.
	\end{aligned}
	\label{equ:mi_input}
	\end{equation}
	The noise level of Noisy Softplus, $\sigma^2$, is the variance of the current, which also can be seen as a weighted sum of the same input $x$ but with different weights:
	\begin{equation}
	\begin{aligned}
		& {s_I^2}_j=\sum_i(\frac{1}{2} w_{ij}^2) (\lambda_{i}\tau_{syn})~, \textrm{  thus}\\
		& \sigma^2_j= \sum_i (\frac{1}{2} w_{ij}^2) x_i~~.
	\end{aligned}
	\label{equ:si_input}
	\end{equation}
	
	
	Noisy Softplus transforms the noisy current with parameters of $(net_j, \sigma_j)$ to the equivalent ANN output $y_j$ , where it can be scaled up by the scaling factor $S$ to the firing rate of SNNs.
	Notice that the calculation of noise level is not necessary for activation functions other than Noisy Softplus, for example, it can be set to a constant for Softplus or 0 for ReLU.
	And the entire artificial spiking neuron model is then generalised to any ReLU/Softplus-like activation functions, See Figure~\ref{Fig:sneuron}.
	
	\begin{figure}[bt]
		\centering
		\includegraphics[width=0.98\textwidth]{pics_sdlm/neuron_o.pdf}
		\caption{Artificial spiking neuron model. }
		\label{Fig:sneuron}
	\end{figure}
	
	
	\subsection{Layered-up Network}
	
	Figure~\ref{Fig:sneuron} shows an complete transformation process of a spiking neuron, which mimics the biological neurons taking and generating spike trains.
	If we move the left end process of $\times \lambda$ to the right hand side, it then forms the same model structure as artificial neurons shown in Figure~\ref{Fig:neuron}: neurons takes $x$ as input and outputs $y$, See Figure~\ref{Fig:tneuron}.
	The process within an artificial neuron is divided to weighted summation and activation, which also applied to the spiking neuron model by combining the scaling factor $S$ and the synaptic time constant $\tau_{syn}$ to activation functions.
	Thus the combined activation function for artificial spiking neurons should be:
	\begin{equation}
	y = f(x) \times S \times \tau_{syn}~~.
	\label{equ:full_act}
	\end{equation}
	\begin{figure}[tbh!]
		\centering
		\includegraphics[width=0.8\textwidth]{pics_sdlm/neuron_t.pdf}
		\caption{Transformed artificial spiking neuron model for ANNs. The combined activation links the firing activity of a spiking neuron to the numerical value of ANNs.}
		\label{Fig:tneuron}
	\end{figure}
	
	
	
	The derivative function used when back propagates is:
	\begin{equation}
	\frac{\partial y}{\partial x} = f'(x) \times S \times \tau_{syn}~~.
	\end{equation}
	

	
	Thus, using this method of ANN-trained SNNs, the activation functions are of lower complexity than Siegert formula, and their corresponding derivative functions can be directly used for back propagation.
	Furthermore, the method enables ReLU-like activation functions for SNN training, thus to improve the recognition accuracy while keeping a relative lower firing rate compared to Sigmoid neurons. 
	Most significantly, the ANN-trained weights are ready for use in SNNs without any transformation, and the output firing rate of any spiking neuron can be estimated in the ANN simulation.
	
	
	\subsection{Fine Tuning}
	The labels of data are always converted to binary values for ANN training.
	It enlarges the disparities between the correct recognition label and the rest, thus to train the network with better classification capability.
	However, SNNs are always active due to the noise of the synaptic current, which drives the neurons to fire even with negative current.
	Thus on the top layer, the neurons which are supposed to be silent still generate sparse spikes.
	Consequently, we train the network as stated above (with any activation function) and then fine tuning it with Noisy Softplus in order to take account of both accuracy and practical network activities of SNNs.
	
	We add a small number (0.01) to all the binary values of the data labels,	then replace the activation function by Noisy Softplus.
	By doing so, it helps the training to loosen the strict objective function to predict exact with binary values.
	Instead, it allows a small offset to the objective.
	The result of fine tuning will be demonstrated in the Section~\ref{sec:iconipResult}.
	An alternative method is to use Softmax function at the last layer, whose aim is the same.
	However, without a limit on the output, it will be easy to reach the highest firing rate of a spiking neuron.
	
\section{Results}
\label{sec:iconipResult}
	A convolutional network model was trained on MNIST,
	%	~\cite{lecun1998gradient}
	a popular database in neuromorphic vision, using the ANN-trained SNN method stated above.
	The architecture contains $28\times28$ input units, followed by two convolutional layers 6c5-2s-12c5-2s, and the 10 output neurons fully connected to last pooling layer represent the classified digit.
	
	The training employed Noisy Softplus units only that all the convolution, average sampling, and the fully-connected neurons use Noisy Softplus function with no bias.
	The parameters of the activation function were calibrated as, $(k=0.30, S=201)$,  for LIF neurons~(see~Tablel~\ref{tbl:pynnConfig}) of $\tau_{syn}=5$~ms.
	The input images were scaled by 100~Hz to present the firing rates of input spikes.
	The weights were updated using a decaying learning rate, 50 images per batch and 20 epochs.
	The ANN-trained weights were then directly applied in the corresponding convolutional SNN without any conversion for recognition tasks.
	
	
	\subsection{Neural Activity}
	Firstly, to validate how well the Noisy Softplus activation fits to the response firing rate of LIF neurons in a real application, we simulated the model on Nest using the Poisson MNIST dataset~\cite{liu2016bench} and the neurons of a convolutional map were observed.
	
		\begin{figure}[tbh!]
		\centering
		\begin{subfigure}[t]{0.8\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/6-1.png}
			\caption{10 input digits as a raster plot}
			\label{Fig:61}
		\end{subfigure}\\
		\begin{subfigure}[t]{0.3\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/6-2.pdf}
			\caption{Pixel firing rates}
			\label{Fig:62}
		\end{subfigure}
		\begin{subfigure}[t]{0.3\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/6-3.pdf}
			\caption{5x5 kernel}
			\label{Fig:63}
		\end{subfigure}
		\begin{subfigure}[t]{0.3\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/6-4.pdf}
			\caption{Output firing rates}
			\label{Fig:64}
		\end{subfigure}
		\caption{Images presented in spike trains convolve with a weight kernel. (a) The $28\times28$ Poisson spike trains in raster plot, representing 10 digits in MNIST. (b) The firing rate of all the 784 neurons of the fourth image, digit `0', is plotted as a 2D image.
		(c) One out of six of the trained kernels ($5\times5$ size) in the first convolutional layer.
		(d) The spike trains plotted as firing rate of the neurons in the convolved 2D map.}
		\label{fig:cnn}
	\end{figure}

	Figure~\ref{fig:cnn} shows a small test of ten MNIST digits presented in Poisson spike trains for 1~s each.
	A trained $5\times5$ kernel (Figure~\ref{fig:cnn}(c)) were convolved with these input digits, and the fourth digit `0' (Figure~\ref{fig:cnn}(b)) and its convolved output of the feature map was shown in Figure~\ref{fig:cnn}(d) as firing rate.
	The output firing rate was recorded during a real-time SNN simulation on NEST, and compared to the modelled activations of Equation~\ref{equ:full_act} in ANNs.
	
	The input $x$ of the network was calculated as Equation~\ref{equ:mi_input}: $x_i=\lambda_i\tau_{syn}$, and so as the weighted sum of the synaptic current (see Equation~\ref{equ:mi_input}), $net_j$ and its variance (see Equation~\ref{equ:si_input}), $\sigma^2_j$.
	With three combined activation functions as Equation~\ref{equ:full_act}:
	\begin{equation}
	\begin{aligned}
	&\textrm{(1) Noisy Softplus:~~}  y_j=k \sigma_j \log [1 + \exp(\frac{net_j}{k \sigma_j})] \times S \times \tau_{syn}~~,  \\
	&\textrm{(2) ReLU:~~ } y_j=max(0, net_j) \times S \times \tau_{syn}~~, \\
	&\textrm{(3) Softplus:~~ } y_j=k \sigma \log [1 + \exp(\frac{net_j}{k \sigma})] \times S \times \tau_{syn}~~, ~~~\sigma=0.3,  
 	\end{aligned}
	\end{equation}	
	we compare the output to the recorded SNN simulations.
	ReLU assumes a non-noise current, and Softplus takes a static noise level thus $\sigma_j$ is not used for either of them, meanwhile Noisy Softplus adapts to noise automatically with $\sigma_j$.
	The experiment took the sequence of 10 digits shown in Figure~\ref{fig:cnn} to the same kernel and the estimated spike counts using Noisy Softplus fit to the real recorded firing rate much more accurately than ReLU and Softplus,  see~\ref{fig:af_compare}.
	Figure~\ref{fig:af_stast} illustrated the statistics of error between the estimation and the recorded firing rate, $err = y_j - \lambda_j$ which formed normal distributions where Noisy Softplus hold the weakest mean (low in abstract) and lowest standard deviation.
	We manually selected a static noise level of 0.45 for Softplus, whose estimated firing rates located roughly on the top slope of the real response activity.
	Which resulted in the higher error than ReLU, since most of the input noisy currents were of relatively low noise level in this experiment.
	
			
	\begin{figure}[tbh!]
		\centering
		\begin{subfigure}[t]{0.9\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/6-5.pdf}
		\end{subfigure}
		\caption{
			Noisy Softplus fits to the neural response firing rate in an SNN simulation.
			The recorded firing rate of the same kernel convolved with 10 images shown in Figure~\ref{fig:cnn} in SNN simulation, comparing to the prediction of activations of Noisy Softplus, Softplus and ReLU.}
		\label{fig:af_compare}
	\end{figure}
	
	Figure~\ref{Fig:out} demonstrates the output firing rates of the 10 recognition neurons when tested with the digit sequence.
	The SNN successfully classified the digits where the correct label neuron fired the most.
	We trained the network with binary labels on the output layer, thus the expected firing rate of correct classification was $1/\tau_{syn}=200$~Hz according to Equation~\ref{Fig:tneuron}.
	The firing rates of the recognition test fell to the valid range around 0 to 200~Hz.
	This shows another advantage of the proposed ANN-trained method that we can constrain the expected firing rate of the top layer, thus to prevent SNN from exploding its firing rate, e.g. 1000~Hz at the most when time resolution of SNN simulation set to 1~ms.
	
		\begin{figure}[tbp!]
			\centering
			\includegraphics[width=0.7\textwidth]{pics_iconip/7.pdf}
			\caption{Output firing rates for recognising 10 hand written digits.}
			\label{Fig:out}
		\end{figure}
	
	\subsection{Recognition Performance}
	Secondly we focus on the recognition performance of the proposed ANN-trained SNN method.
	Before we reach there, it is significant to see the learning capability of the proposed activation function, Noisy Softplus.
	We compared the training using ReLU, Softplus, and Noisy Softplus by their loss during training averaged over 3 trials, see Figure\ref{fig:training}.
	ReLU learned the fastest and reached the lowest loss, thanks to its steepest derivative and none-noise assumption.
	To the contrary, Softplus accumulated spontaneous firing rates layer by layer and its derivative may experience gradients vanish during back propagation, which result in a more difficult training.
	Noisy Softplus performed in between in terms of loss and learning speed.
	However, the loss stabilised the fastest, which means a possible shorter training time.
	\begin{figure}[tbp!]
		\centering
		\includegraphics[width=0.7\textwidth]{pics_iconip/8.pdf}
		\caption{Loss during training.}
		\label{Fig:loss_ns}
	\end{figure}
%	The trained networks were scaled to SNNs and compared on recognition rates, 93.34\%, 96.43\% and 97.03\% with a conversion loss of 4.76\%, 0.91\% and 0.74\%.
	
	\begin{figure}[tbp!]
		\centering
		\includegraphics[width=0.8\textwidth]{pics_iconip/9-2.pdf}
		\caption{Classification accuracy compared among trained weights of Noisy Softplus, ReLU, Softplus on DNN, SNN and fine-tuned SNN.}
		\label{Fig:result_bar}
	\end{figure}
	\begin{table}[ph!] 
		\caption{Comparisons of ANN-trained convolutional neural models on original DNN, NEST simulated SNN, and SNN with fine-tuned model.}
		\begin{center}
			\bgroup
			\def\arraystretch{2.5}
			\begin{tabular} {r c  c c c c c c c c c}
				%First line
				Trial No.
				&\multicolumn{3}{c}{1} 
				&\multicolumn{3}{c}{2}
				&\multicolumn{3}{c}{3}\\
				\hline
				Model
				& DNN & SNN &FT
				& DNN & SNN &FT
				& DNN & SNN &FT\\
				\hline
				\textbf{Noisy Sofplus}
				& 1.91 & 2.76 &2.45
				& 1.79 & 2.56 &2.19
				& 1.76 & 2.55 &2.10\\
%				& 98.09 & 97.24 &97.55
%				& 98.21 & 97.44 &97.81
%				& 98.24 & 97.45 &97.90\\
%				\hline
				\textbf{ReLU}
				& 1.36 & 2.03 &1.88
				& 1.46 & 2.28 &2.00
				& 1.36 & 2.25 &2.12\\
%				& 98.64 & 97.97 &98.12
%				& 98.54 & 97.72 &98.00
%				& 98.64 & 97.75 &97.88\\
%				\hline
				\textbf{Sofplus}
				& 2.30 & 5.66 &3.91
				& 2.75 & 5.22 &3.55
				& 2.42 & 6.62 &3.87\\
%				& 97.70 & 94.34 &96.09
%				& 97.25 & 94.78 &96.45
%				& 97.58 & 93.38 &96.13\\
				\hline
			\end{tabular}
			\egroup
			\label{tbl:ns_result}
		\end{center}
	\end{table}
	
	
	As it is a major concern in neuromorphic vision, the recognition performance over short response times is also estimated in Figure~\ref{fig:ca_time}(a).
	\begin{figure}[bt!]
		\centering
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/8-2.pdf}
		    \caption{Before fine tuning}
		\end{subfigure}
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/8-3.png}
		    \caption{After fine tuning.}
		\end{subfigure}

		\caption{The classification accuracy over short response times, with (a) trained weights before fine tuning, and (b) after fine tuning.}
		\label{fig:ca_time}	
	\end{figure}

	\subsection{Power Consumption}
\section{Summery}
	The Spiking Neural Network (SNN) has not achieved the recognition/classification performance of its non-spiking competitor, the Artificial Neural Network(ANN), particularly when used in deep neural networks.
	How to equip SNNs with equivalent recognition capability as DNNs' thus to take advantages of their neuromorphic implementation attracts increased attention both in the neuromorphic community and the deep learning industry.
	%TODO the scalability, power efficiency and real-time sensor interactive with the environment 
	%	The mapping of a well-trained ANN to an SNN is a hot topic in this field, especially using spiking neurons with biological characteristics.
	This Chapter proposes a new biologically-inspired activation function, Noisy Softplus, which is well-matched to the response function of LIF (Leaky Integrate-and-Fire) neurons.
	A convolutional network (ConvNet) was trained on the MNIST database with Noisy Softplus units and converted to an SNN while maintaining a close classification accuracy.
	This result demonstrates the equivalent recognition capability of the more biologically-realistic SNNs and bring biological features to the activation units in ANNs.
	%TODO rewording
	The proposed method uses the same conventional ANN training only, and the off-line trained model can be directly applied to SNN without conversion.
	
	The biologically-inspired activation function, Noisy Softplus, adapts to the noise level of input currents automatically, and is the first attempt to map activation units accurately to the firing response of LIF neurons.
	Noisy Softplus not only brings more biological features to the activation function, but also proves capable of performing well in a spiking ConvNet recognition task.
	The spiking version of Noisy Softplus wins on accuracy over the sigmoid neuron, compared to the result~\cite{Stromatias2015scalable} of using Siegert units.
	As a result of its more accurate mapping, Noisy Softplus outperforms Softplus.
	
	The Noisy Softplus activation function proposed here is based on LIF neurons with biological characteristics, and is the first attempt to map a spiking neural response accurately to the activation unit of an ANN.
	The resulting classification accuracy was tested on a spiking ConvNet; the performance was close to that of the original ConvNet, and was better than using Softplus.
	This study brings a significant biological feature, noise, to the activation units of an ANN, in the hope of promoting research into noise-based computation.
	
	Future work on SNNs will include constraints during training to limit the function within the active range, which is equivalent to constraining the maximum firing rate of an LIF neuron.
	As a result there should be no need for the scaling process after training.
	For more accurate mapping, the scale factor $k$ should be (numerically) derived to avoid calibration.
	In ANNs, it could be useful to study noise as extra information to be gathered by Softplus activation to enhance classification.