\chapter{Conclusion and Future Work}
\label{cha:conc}
%Neuromorphic engineering has been developed for large-scale, energy-efficient, simulations on networks comprising biologically-plausible spiking neurons.
%Nevertheless the Spiking Neural Network (SNN) has not achieved the cognitive capability of its non-spiking counterpart, the Artificial Neural Network (ANN).
%Deep Learning techniques in the field of ANN have driven simple rate-based artificial neurons to surpass human-level capabilities in cognitive tasks which humans used to dominate. 
%Chapter~\ref{cha:bkg} revealed the special features of spiking neurons that differ from the neurons of conventional ANNs.
%%the core reason of the difference in cognitive abilities that the time-driven neural dynamics of spiking neuron models intrinsically differ from the abstracted rate-driven artificial neurons.
%The fundamental difference between a biologically-plausible spiking neuron and an abstract artificial one raises the research problem: understanding how to operate and train biologically-plausible SNNs to close the gap in cognitive capabilities between SNNs and ANNs on Artificial Intelligence~(AI) tasks.
%
%Embedding Deep Learning mechanisms for training SNNs may provide an answer to the problem.
%In Chapter~\ref{cha:dnn}, the most popular and influential Deep Learning models and mechanisms were introduced, and three of them were demonstrated in detail and later successfully applied on SNNs.
%Most significantly, in Chapter~\ref{cha:Conv} and Chapter~\ref{cha:sdlm} we solved the main research problem by proposing two effective SNN training methods which exhibited learning capabilities equivalent to the non-spiking ANN models. 
%%One of them is generalised and off-line where the connections of some deep, forward SNN are tuned on an equivalent ANN and transferred back to the SNN;
%%and the other is formalised and on-line where the training takes place on an SNN directly and the tuned network performs equivalently to a Restricted Boltzmann Machine~(RBM) or an Autoencoder(AE).
%In addition, Chapter~\ref{cha:bench} provided the neuromorphic community with a uniform dataset to evaluate SNNs' performance at both model and hardware level to provide meaningful comparisons between these proposed SNN models and other existing methods within this rapidly advancing field of NE.%, thus to answer the research problem with strong evidence comparing to ANNs.

%Energy-efficient neuromorphic hardware platforms have been successfully contributed to large-scale simulations of biologically-plausible Spiking Neural Networks~(SNNs) for brain understanding, but they are still far from intelligent to achieve Neuromorphic Cognition.
%Meanwhile, Deep Learning techniques in the field of Artificial Neural Network~(ANN) have driven simple rate-based artificial neurons to surpass human-level capabilities in cognitive tasks, e.g. vision.
%Thus, to equip these powerful brain-inspired neuromorphic computers with cognitive capabilities, the thesis aims to understand how to close the gap of the performance between SNNs and ANNs on Artificial Intelligence~(AI) tasks. 


In this chapter, we will confirm the hypotheses proposed in Chapter~\ref{cha:intro} to answer the thesis question: how to close the gap of the cognitive capabilities between Spiking Neural Networks~(SNNs) and Artificial Neural Networks~(ANNs) on Artificial Intelligence~(AI) tasks.
This involves introducing sub-topics on how we arrived at this confirmation, what are the main contributions, and how this work challenges previous research.
This will be followed by a statement of the current limitations of our study, potential methods for tackling these limitations, and directions for further research.

%One paragraph stating what you researched and what your original contribution to the field is…then break into sections
%Be enthused by the prospect of writing your conclusion… “It’s downhill from here…!”

%The primary achievements of the thesis are the learning methods both off-line and on-line for SNNs, which close the gap of cognitive capability between SNNs and ANNs.
%The other achievements contributes to the concerns of the feasibility of neuromorphic hardware platforms and the performance evaluation.

\section{Confirming Research Hypotheses}
\paragraph{1. Deep SNNs can be successfully and simply trained off-line where the training takes place on equivalent ANN and the tuned weights then transferred back to the SNNs, thus making them as competent as ANNs in cognitive tasks.}
This hypothesis aims to generalise a simple but effective method for off-line SNN training.

The key problem of such an off-line method lies in the transformation of ANN models to SNNs.
In Chapter~\ref{cha:Conv}, we broke down the problem into two smaller parts and solved them with proposed novel activation functions: Noisy Softplus~(NSP) and the Parametric Activation Function~(PAF).
NSP accurately models the output firing activity in response to the current influx of a Leaky Integrate-and-Fire~(LIF) neuron.
% with a conventional activation function of abstract values.
It tackles the first problem of modelling discrete, spike-based neural computation with continuous activation functions of abstract values in ANNs.
Next, PAF addresses the other problem of mapping these numerical values to concrete physical units in SNNs: input currents in nA and output firing rates in Hz.
Therefore, a standard LIF neuron of biologically-valid parameters can be represented as a PAF-NSP neuron in ANN.
Consequently, an ANN comprised of PAF-NSP neurons can work equivalently to an SNN of LIF neurons; and the weights of this ANN model could be transferred to the SNN without any transformation.
Moreover, the training of PAF-NSP neurons can be generalised to a PAF version of conventional activation functions, which greatly reduces the computational complexity.

%Moreover, an important feature of accurately modelling LIF neurons in ANNs is the acquisition of spiking neuron firing rates. These will aid deployment of SNNs in neuromorphic hardware by providing power and communication estimates, which allow better usage or customisation of the platforms.

%The simple off-line training method can be summarised in three steps:
%firstly, estimate parameter $p$ for the PAF, $y = p \times f(x)$, using NSP;
%secondly, use a PAF version of a conventional activation function, e.g. Rectified Linear Unit~(ReLU), to train an equivalent ANN;
%thirdly, transfer the tuned weights directly into the SNN.
At one hand, this research contributes to the NE community a simple off-line SNN training method.
It enables any forward SNN to be modelled and trained on an equivalent ANN, thus resolves the difficulties of transforming ANN models to SNNs.
At the other hand, the method is generalised in terms of spiking neural models and hardware platforms, since it works on standard LIF neurons which are supported by most of the neuromorphic hardware.
Hence, AI engineers can not only implement their ANN models on NE platforms without knowledge of SNN or hardware-specific programming;
but also benefit from neuromorphic hardware in many ways: such as energy-efficiency, biological realism, low latency and real-time processing.


%This method involves the least computational complexity while performing most effectively among existing algorithms.

%In Chapter~\ref{cha:Conv} we proposed a generalised SNN training method to train an equivalent ANN and transfer the trained weights back to SNNs.
%This training procedure consists of two simple stages: first, estimate parameter $p$ for the Parametric Activation Function~(PAF: $y = p \times f(x)$) with the help of the activation function we proposed, Noisy Softplus~(NSP), and second, use a PAF version of the conventional activation functions for ANN training. % can be generalised to activation units other than NSP.
%%The training of a SNN model is exactly the same as ANN training, and 
%The trained weights can be used directly in the SNN without any further transformation.
%This method requires the least computation complexity while performing most effectively among existing algorithms.
%% and even more straight-forward than the other ANN offline training methods which requires an extra step of converting ANN-trained weights to SNN's.

Among the existing approaches, this method is the first and only that unifies the representation of neurons in ANNs and the spiking ones in SNNs using abstract activation functions, while some other approaches~\citep{Jug_etal_2012,hunsberger2015spiking} directly handle physical units.
In terms of modelling accuracy, NSP not only takes the noise of the current influx into account which is missing in soft LIF~\citep{hunsberger2015spiking}, but also includes coloured noise where Siegert formula~\citep{Jug_etal_2012} only works on white noise.
Regarding simplicity, this training method performs effectively using Rectified Linear Unit~(ReLU), however, other studies~\citep{Jug_etal_2012,hunsberger2015spiking} suffer from high computational complexity.
In addition, thanks to the accurate modelling of PAF, the trained weights are ready to use on SNNs, while previous approaches ~\citep{cao2015spiking,diehl2015fast} require an extra step to make the trained weights adaptive to the SNN.
Moreover, comparing to the requirements of specific neural models~\citep{cao2015spiking,diehl2015fast} and the hardware-specific training methods~\citep{diehl2016conversion,diehl2016truehappiness,esser2015backpropagation}, our approach is generalised to target standard LIF neurons and neuromorphic hardware platforms.

%This proposed SNN training method is simpler and even more straightforward than the previous ANN-based training approaches~\citep{cao2015spiking,diehl2015fast} which require an extra step of converting the trained weights for use at the SNN.
%%In addition, the normalisation algorithm~\citep{diehl2015fast} proposed for weight transformation only works for simple integrate-and-fire neurons, and cannot be generalised to biologically-plausible neurons.
%More importantly, it addresses the problems of inaccurate modelling and high computational complexity of existing approaches~\citep{Jug_etal_2012,hunsberger2015spiking} using LIF neurons.
%Moreover, the novel activation function, Noisy Softplus, tackles all of the problems raised by the Siegert formula which has been used to model sigmoid-like spiking neurons~\citep{Jug_etal_2012} as follows: 
%\begin{itemize}
%	\item NSP accounts for the time correlation of the noisy synaptic current, e.g. $\tau_{syn}$, thus better fitting the actual response firing rate of an LIF neuron. % compared to Siegert function.
%	
%	%		\item easily applied to any training method, for example BP, thanks to its derivative function defined in Equation~\ref{equ:logist}.
%	
%	\item The calculation of NSP and its derivative involves no more calculations than the Softplus function, except for double the computation on the weighted sum of its input ($net$ and $\sigma$ in Equations~\ref{equ:mi_input}).
%	They are also much simpler than the Siergert function, which saves training time and energy.
%	
%	\item As one of the ReLU-liked activation functions, the output firing rate seldom exceeds the working range of an LIF neuron.
%	For example the firing rates were around 0-200~Hz in the Convolutional Neural Network~(ConvNet) model in Chapter~\ref{cha:Conv}, while a sigmoid neuron is only active when it fires faster than half of its maximum rate~(1K Hz).
%	
%	\item The learning performance of Noisy Softplus lies between that of Softplus and ReLU, which outperforms most of the other popular activation functions.
%\end{itemize}
%In summary, the proposed off-line training method addresses the problems of inaccurate modelling and high computational complexity of existing approaches.


To validate the cognitive capability of SNN models trained by this proposed method, we compared the classification accuracy of the spiking ConvNets to the non-spiking ANNs.
The performance was nearly equivalent, and the best classification accuracy achieved 99.07\% on the MNIST task which outperformed state-of-the-art SNN model of LIF neurons~\citep{hunsberger2015spiking} and equalled the best result using simplified IF neurons~\citep{diehl2015fast}.


We confirm the hypothesis with the first contribution of this thesis: providing NE community a simple, but effective, generalised, off-line deep SNN training method.

\paragraph{2. Unsupervised Deep Learning modules can be trained on-line on SNNs with biologically-plausible synaptic plasticity to demonstrate a learning capability as competent as ANNs.}
This hypothesis targets the formalising of a local learning rule based on synaptic plasticity for unsupervised, event-based, biologically-plausible training of deep SNNs. %in order to catch up with the recognition performance of ANNs.

%In Chapter~\ref{cha:sdlm} we proposed unsupervised Deep Learning algorithms to train SNNs purely with event-based local STDP, using a Spike-based Rate Multiplication method (SRM).
%The proposed Spike-based Rate Multiplication~(SRM) method represents the product of numerical values used in these unsupervised Deep Learning techniques with rate multiplication.
%The SRM then transforms the rate multiplication to the number of coincident spikes emitted from a pair of rate-coded spike trains, and the simultaneous events can be captured by the weight change of the biologically-plausible learning rule: Spike-Time-Dependant-Plasticity~(STDP).
%During the research we encountered the problem introduced by correlated spikes, and proposed solutions to decorrelate pairs of spike trains.

Instead of transforming off-line trained ANN models to SNNs, on-line methods face the problem of translating numerical computations for training Deep Learning modules into synaptic plasticity rules.
Multiplication is the core operation in the unsupervised Deep Learning algorithms, Autoencoders~(AEs) and Restricted Boltzmann Machines~(RBMs): $\Delta w \propto ab-cd$.
In Chapter~\ref{cha:sdlm} we found that the product of two numerical values could be represented with rate multiplication of a pair of rate-coded spike trains;
and the proposed formalised Spike-based Rate Multiplication~(SRM) method precisely transformed the product of rates to the number of simultaneous spikes generated from a pair of connected spiking neurons.
Most importantly, the simultaneous events were captured by the weight change of the synaptic connection using the Spike-Timing-Dependent Plasticity~(STDP) learning rule.
Therefore, the SRM tackles the problem of translating the weight tuning from numerical computations to event-based, biologically-plausible learning rules in SNNs.

Spiking AEs and RBMs can be trained with SRM by sharing the synaptic weights between $ab$ and $cd$, and applying symmetric learning rate $\pm \eta$ on the weight change respectively.
The performance approaches the same, sometimes even superior, classification and reconstruction capabilities compared to their equivalent non-spiking models.
In addition, the numerical analysis of the proposed algorithm accurately estimates the learning rate $\pm \eta$, thus closely mimicking the learning behaviour of the AE and RBM modules, and improves the learning performance compared to existing methods.


%It is crucial to provide deep SNNs with effective on-line training algorithms, not only for building successful spike-based object recognition applications, but also for better power efficiency and scalability when training on neuromorphic hardware.



%This on-line training method achieves better performance than existing algorithms and approaches the same, sometimes superior performance of the equivalent non-spiking methods.

Thanks to the formalisation of these proposed learning algorithms with numerical analysis, the classification results (94.72\% for SAE and 94.35\% for SRBM) have outperformed other existing SAE and SRBM models.
The first on-line training algorithm proposed by~\citet{neil2013online} ignored the mathematical analysis thus achieved its best classification performance only at 81.5\%.
\citet{neftci2013event} conducted training on a recurrent network which was more biologically plausible but required a global signal to control the direction of the synapses and resulted in a worse classification of 91.9\% even with a bigger network.
Moreover, the proposed method is the first to exploit novel techniques to decorrelate the spike trains since the correlation brings down the learning performance rapidly after it peaks.
Meanwhile, other on-line training methods~\citep{neftci2016stochastic,neftci2017event} suffer from the manual termination of the learning.
Furthermore, in theory our method works on any spiking neuron and requires a simple rectangular STDP, thus can be easily implemented on general neuromorphic hardware.
However, it will be more difficult for other existing approaches~\citep{neftci2013event,neftci2016stochastic,neftci2017event} since they asks for either specific neural/synaptic models or extra external signals to control the learning.

Our second contribution is the formalisation of an STDP-based unsupervised learning algorithm for spiking AE~(SAE) and spiking RBM~(SRBM).
%These training methods realise on-line, event-based, biologically-plausible learning on spiking deep architectures in an unsupervised fashion.
The promising results of equivalent or even superior classification and reconstruction capabilities of SAEs and SRBMs compared to their conventional ANN-based methods confirms the hypothesis that SNNs have learning ability as competent as deep ANNs.

\paragraph{3. A new set of spike-based vision datasets can provide resources and corresponding evaluation methodology to support objective comparisons and measure progress within the rapidly advancing field of NE.}
This hypothesis is expected to provide a unified spiking version of a commonly-used dataset and a complementary evaluation methodology to assess the performance of SNN algorithms.

%A dataset and the corresponding evaluation methodology for comparisons of Neuromorphic Vision.
%This dataset also made the comparison of SNNs with conventional recognition methods possible by using converted spike representations of the same vision databases.
%As far as we know, this was the first attempt at benchmarking neuromorphic vision recognition by providing public a spike-based dataset and evaluation metrics.
%
%
%The first version of the dataset is published as NE15-MNIST, which contains four different spike representations of the MNIST stationary hand-written digit database.
%The Poissonian subset is intended for benchmarking existing rate-based recognition methods.
%The rank-order coded subset, FoCal, encourages research into spatio-temporal algorithms on recognition applications using only small numbers of input spikes.
%Fast recognition can be verified on the DVS recorded flashing input subset, since just 30~ms of useful spike trains are recorded for each image.
%Looking forward, the continuous spike trains captured from the DVS recorded moving input can be used to test mobile neuromorphic robots.
%\citep{orchard2015convert} have presented a neuromorphic dataset using a similar approach, but the spike trains were obtained with micro-saccades.
%This dataset aims to convert static images to neuromorphic vision input, while the recordings of moving input in our paper are intended to promote position-invariant recognition.
%Therefore, the datasets complement each other.
%
%The proposed complementary evaluation methodology is essential to assess both the model-level and hardware-level performance of SNNs.
%In addition to classification accuracy, response latency and the number of synaptic events are specific evaluation metrics for spike-based processing.
%Moreover, it is important to describe an SNN model in sufficient detail to share the network design, and relevant SNN characteristics were highlighted in the paper.  
%%For a neural network model, its topology, neuron and synapse models, and training methods are major descriptions for any kind of neural networks, including SNNs.
%%While the recognition accuracy, network latency and also the biological time taken for both training and testing are specific performance measurements of a spike-based model.
%The network size of an SNN model that can be built on a hardware platform will be constrained by the scalability of the hardware.
%Neural and synaptic models are limited to the ones that are physically implemented, unless the hardware platform supports programmability.
%Any attempt to implement an on-line learning algorithm on neuromorphic hardware must be backed by synaptic plasticity support.
%Therefore running an identical SNN model on different neuromorphic hardware exposes the capabilities of such platforms.
%If the model runs smoothly on a hardware platform, it then can be used to benchmark the performance of the hardware simulator in terms of simulation time and energy usage.
%Classification accuracy (CA) is also a useful metric for hardware evaluation because of the limited precision of the membrane potential and synaptic weights.
%
%%Although spike-based algorithms have not surpassed their non-spiking counterparts in terms of recognition accuracy, they have shown great performance in response time and energy efficiency.
In Chapter~\ref{cha:bench} we presented a dataset which contains four different spike representations of the MNIST stationary hand-written digit database. % rate-based code, rank-order code, DVS recorded flashing and moving inputs. 
The Poissonian subset is intended for benchmarking existing rate-based recognition methods.
The rank-order coded subset, FoCal, encourages research into spatio-temporal algorithms for recognition applications using only small numbers of input spikes.
Fast recognition can be verified on the DVS recorded flashing input subset, since just 30~ms of useful spike trains are recorded for each image.
Looking forward, the continuous spike trains captured from the DVS recorded moving input can be used to test mobile neuromorphic robots.
\citet{orchard2015convert} presented a neuromorphic dataset using a similar approach, but the spike trains were obtained with micro-saccades.
This dataset aims to convert static images into neuromorphic vision input, while the recordings of moving input in our paper are intended to promote position-invariant recognition.
Therefore, the datasets complement each other.

The proposed complementary evaluation methodology is essential to assess both the model-level and hardware-level performance of SNNs.
In addition to classification accuracy, response latency and the number of synaptic events are specific evaluation metrics for spike-based processing.
%	This work contributes to (1) promoting meaningful comparisons between algorithms and neuromorphic platforms in the field of NE, (2) allowing comparison with conventional image recognition methods, (3) providing an assessment of the state of the art in spike-based visual recognition, and (4) helping researchers identify future directions and advance the field.
Moreover, it is important to describe an SNN model in sufficient detail to share the network design. 
The network size of an SNN model that can be built on a hardware platform will be constrained by the scalability of the hardware.
Neural and synaptic models are limited to those that are physically implemented, unless the hardware platform supports programmability.
Any attempt to implement an on-line learning algorithm on neuromorphic hardware must be backed by synaptic plasticity support.
Therefore running an identical SNN model on different neuromorphic hardware exposes the capabilities of such platforms.
If the model runs smoothly on a hardware platform, it then can be used to benchmark the performance of the hardware simulator in terms of simulation time and energy usage.
Classification accuracy is also a useful metric for hardware evaluation because of the limited precision of the membrane potential and synaptic weights.


A third contribution of the thesis provides the NE community with a dataset and its corresponding evaluation methodology for comparisons of SNN models and NE platforms.
The published NE15-MNIST dataset contains spike-based representations of the popular hand-written digits database, MNIST.
Moreover, the carefully selected evaluation metrics highlight the strengths of spike-based vision tasks and the dataset design also promotes research into rapid and low energy recognition (e.g. flashing digits).
The successful baseline test of a benchmark system has been evaluated using the Poissonian subset of the NE15-MNIST dataset, which validates the feasibility of the database and its evaluation.
There, we confirm the hypothesis that the dataset provides resources and supports fair comparisons among SNN models and their hardware implementations.

%
%As far as we know, this has been the first attempt at benchmarking neuromorphic vision recognition by providing public a spike-based dataset and evaluation metrics.
%In accordance with the suggestions from~\citep{tan2015bench}, the evaluation metrics highlight the strengths of spike-based vision tasks and the dataset design also promotes the research into rapid and low energy recognition (e.g. flashing digits).
%A benchmark system has been evaluated using the Poissonian subset of the NE15-MNIST dataset.
%%The models were described and their performance on accuracy, network latency, simulation time and energy usage were presented.
%These example benchmarking system has demonstrated a recommended way of using the dataset, describing the SNN models and evaluating the system performance.
%%They provide a baseline for comparisons and encourage improved algorithms and models to make use of the dataset.
%The case study has provided baselines for robust comparisons between SNN models and their hardware implementations.



\section{Future Work}
Though the research aim has been mostly achieved by the work presented in this thesis, some limitations still remain to be addressed in the future.
In addition, this work has inspired new directions for future work to continue and expand the current research.

\subsection{Off-line SNN Training}

\paragraph{Supporting software tools.}
The current limitation prohibiting the off-line SNN training method from wide use lies in the lack of supporting tools.
This requires the development of a set of software and libraries:

\begin{itemize}
	\item parameter calibration of $p$ for PAF given LIF neuron configurations, which involves automatic SNN simulations with different levels of current and noise, followed by curve fit with the activation function NSP. 
%	Furthermore, numerical analysis is considered to present coloured noise introduced by 1~ms time resolution and the synaptic time constant $\tau_{syn}$, refer to Section~\ref{sec:response_func}, thus to model $p$ with biological parameters of a LIF neuron instead of parameter calibration.
	
	\item supporting libraries for SNN training in popular Deep Learning platforms, e.g. Tensorflow~\citep{tensorflow2015-whitepaper}, which will include the proposed activation functions NSP and PAF in the ANN models.
	
	\item a unified template to describe any ANN model and an automation tool that reads platform-dependent trained models into the designed template.
	The tool will not only help to translate ANN models to SNNs, but also contribute to the cross-platform transfer learning and use of pre-trained models in Deep Learning.
	
	\item a translation tool that converts the tuned ANN models into SNNs described in the PyNN~\citep{davison2008pynn} language, thus the SNN model can run on any software simulator or neuromorphic hardware as long as PyNN is supported.
\end{itemize}

\paragraph{Recurrent Neural Networks~(RNNs).}
It is difficult to make SNNs work with recurrent architectures, since a spiking neuron simultaneously takes input from both the lower level on the forward path and the upper level on the feedback path.
However, in ANNs the forward and feedback paths work alternately on separate steps.
Therefore, the proposed SNN training method only applies to feed-forward networks.
One of the major future goals is to propose a general method to run recurrent SNNs which perform equivalently to RNN models.

\paragraph{Applications.}
The simple off-line SNN training method has enabled and encouraged interesting applications to run in SNNs.

\begin{itemize}
	\item Ensemble models~\citep{rokach2010ensemble} have been trained with NSP neurons and will be transferred onto SpiNNaker system~\citep{furber2014spinnaker} to take the advantages of energy-efficiency and massively-parallel processing.
	\item Speech recognition of simulated cochlea generated spikes has achieved a promising accuracy at the initial test-idea stage.
	The next step is to implement the model entirely on neuromorphic hardware including the cochlea and the SNN, thus to run it in real time.
	\item An 18-layer residual network~\citep{he2016deep} has been trained for the task of recognising human facial expressions using the KDEF dataset~\citep{lundqvist1998karolinska}.
	Notably, this is, so far, the deepest network trained with NSP and the recognition performance~(94.95\%) has outperformed ReLU~(92.45\%).
	It will be interesting to analyse the differences of recognition accuracy and robustness using NSP and other activation functions.
	Thus it may answer the question how the brain delivers strong cognitive ability with stochastic and noisy signals.
 	\item A further goal is to implement deep SNNs fit for ImageNet~\citep{deng2009imagenet} tasks, which will also require modelling various functions of Deep Learning on spiking neurons, such as max-pooling and batch normalisation. 

\end{itemize}


\subsection{On-line Biologically-Plausible Learning}
\paragraph{Beyond rate coding.}
Although rate coding has shown good performance on training spiking Deep Learning modules on-line, time-based coding and rank-order coding which carry more information per spike, are expected to have better or faster learning capabilities~\citep{gautrais1998rate}.
We have proposed a similar on-line learning algorithm for training precise-timing based spiking AEs and RBMs, which although still in the test-idea stage, the prototype has shown much faster learning speed than the rate-coding mechanism.

\paragraph{Backpropagation alternatives.}
The STDP rule usually works locally with a teaching signal in cognitive tasks, however, error backpropagation~(BP) does not provide the targets for all the hidden units of a network.
Thus, BP with gradient descent is believed to be difficult to transfer to SNNs and alternative methods with local training algorithms are preferred.
Despite the success of greedy layer-wise training of AEs and RBMs, Random Back-Propagation~(RBP) is also an alternative to backward targets with fixed random weights for hidden layers.
Therefore, RBP fits to the local learning rules of synaptic plasticity in SNNs.
Initial work~\citep{samadi2017deep,neftci2017event} has shown that these random feedback weights work effectively to replace precise BP.  
In the future, we will continue the investigation of the area of on-line training on deep SNNs.

\paragraph{Biologically-plausible Reinforcement Learning.}
The modulation of STDP by a third factor such as dopamine has potentially interesting functional consequences that turn STDP from unsupervised learning into a reward-based learning paradigm~\citep{izhikevich2007solving} which addresses Reinforcement Learning.
Merging advanced neuroscience findings and Deep Learning mechanisms onto on-line SNN training will be the trend for future work.


\paragraph{State-of-the-art Performance.}
Synaptic Sampling Machines (S2Ms)~\citep{neftci2016stochastic} employing a dropout~\citep{srivastava2014dropout} mechanism which hugely improved the performance on MNIST tasks from 91.9\% to 95.6\%.
Thus applying novel Deep Learning techniques for SNN training is also in the future work to improve the state-of-the-art performance..

\paragraph{Neuromorphic Hardware.}
On-line training is a necessary means towards Neuromorphic Cognition where the hardware computers learn while they operate.
The main concerns are the learning speed and the power consumption, which will be compared to off-line training methods and conventional ANN training.
It is still an open question that how to perform the human-level cognition and at the same time achieve the low power cost by on-line learning.



\subsection{Evaluation on Neuromorphic Vision}

\paragraph{Face recognition dataset.}
%The database proposed in Chapter~\ref{cha:bench} will be expanded by converting more popular vision datasets to spike representations.
As mentioned in Section~\ref{sec:chapt6_intro}, face recognition has become a hot topic in SNN approaches, however there is no unified spike-based dataset to benchmark these networks.
Thus, the next step is to include face recognition databases.
While viewing an image, saccades direct high-acuity visual analysis to a particular object or a region of interest and useful information is gathered during the fixation of several saccades in a second.
It is possible to measure the scan path or trajectory of the eyeball and those trajectories show particular interest in eyes, nose and mouth while viewing a human face~\citep{yarbus1967eye}.
Therefore, our plan is also to embed modulated trajectory information to direct the recording using DVS sensors to simulate human saccades.

\paragraph{Converting images to spikes.}
%There will be more methods and algorithms for converting images to spikes.
Although Poisson spikes are the most commonly used external input to an SNN system, there are several \textit{in-vivo} recordings in different cortical areas showing that the inter-spike intervals (ISI) are not Poissonian. 
Thus~\citet{deger2012statistical} proposed new algorithms to generate superposition spike trains of Poisson Processes with Dead-time (PPD) and Gamma processes.
Including novel spike generation algorithms in the dataset is one aspect of future work.

\paragraph{Invariant object recognition}
%While the major stumbling crux of the computer object recognition systems lies in the invariance problem.
Each encounter of an object on the retina is unique, because of the illumination (lighting condition), position (projection location on the retina), scale (distance and size), pose (viewing angle), and clutter (visual context) variabilities.
The brain, however, recognises a huge number of objects rapidly and effortlessly even in cluttered and natural scenes.
To explore invariant object recognition, the dataset will include the NORB (NYU Object Recognition Benchmark) dataset~\citep{lecun2004learning}, which contains images of objects that are first photographed in ideal conditions and then moved and placed in front of natural scene images.

\paragraph{Video processing}
Action recognition will be the first problem of video processing to be introduced in the dataset.
The initial plan is to use the DVS retina to convert the KTH and Weizmann benchmarks to spiking versions.
Meanwhile, providing a software DVS retina simulator to transform frames into spike trains is also on the schedule.
By doing this, a huge number of videos, such as those in YouTube, can be converted automatically into spikes, therefore providing researchers with more time to work on their own applications.

\paragraph{Sharing and collaboration}
Overall, it is impossible for the dataset proposers to provide enough datasets, converting methods and benchmarking results, thus we encourage other researchers to contribute to the dataset allowing future comparisons using the same data source.
They can also share their spike conversion algorithms by generating datasets to promote the corresponding recognition methods.
Neuromorphic hardware owners are welcome to provide benchmarking results to compare their system's performance.
\section{Closing Remarks}
%conclusion of the concolusion
%Be enthused by the prospect of writing your conclusion… “It’s downhill from here…!”
%what are the all contributions, and the significance
%what are the furture work-optional

%The concluding chapter reflects the research question and hypotheses raised in the first Chapter, highlights the main contributions of the work, discusses the work in the context of literature, and propose potential methods to tackle existing limitations and promising directions for future work.
%This thesis has achieved closure of the gap between the cognition capability of SNN to ANN, and brightly paved the way for further study to improve and understand the learning performance of these biologically-plausible spiking units.

Energy-efficient neuromorphic hardware platforms have been successfully contributed to large-scale simulations of biologically-plausible SNNs for brain understanding, but they are still far from intelligent to achieve Neuromorphic Cognition.
Meanwhile, Deep Learning techniques in the field of ANN have driven simple rate-based artificial neurons to surpass human-level capabilities in cognitive tasks, e.g. vision.
Thus, to equip these powerful brain-inspired neuromorphic computers with cognitive capabilities, this thesis contributes to close the gap of the performance between SNNs and ANNs on AI tasks.

One of the major contributions is a simple off-line SNN training method which models and trains any feed-forward SNN on an equivalent ANN, and enables the trained weights work equivalently on the SNN without any conversion.
It significantly simplifies the development of AI applications on neuromorphic hardware thanks to the simple training process and the use of standard LIF neurons which are supported by most neuromorphic hardware platforms.
The success of the generalised off-line method paves the way to energy-efficient AI on mobile devices and huge computer clusters.


Taking one step towards Neuromorphic Cognition, the on-line learning method has also been investigated to train Deep Learning modules on SNNs.
The proposed method tackles the problem of accurately translating the weight tuning of AEs and RBMs from numerical computations to event-based, biologically-plausible STDP rules in SNNs.
It leads the future work to formalise event-based learning to closely mimic the numerical computations in conventional ANNs.
The development of on-line training will equip the neuromorphic computers with genuine learning capabilities.

The last but not least, this work contributes to the NE community with a uniform dataset to evaluate SNNs' performance at both model and hardware level.
The corresponding evaluation methodology will promote meaningful comparisons between these proposed SNN models and other existing methods within this rapidly advancing field.
Moreover, we hope to provide objective comparisons between conventional ANNs and SNNs, in order to give prominence to low latency and energy consumption on neuromorphic hardware.

