\chapter{Benchmarking Neuromorphic Vision}
\label{cha:bench}


The last two chapters have answered the main research question of this thesis \protect\TLSins{showing} that Spiking Neural Networks (SNNs) can be trained both on-line and off-line to achieve the equivalent cognitive capabilities of Artificial Neural Networks~(ANNs).
To provide meaningful comparisons between theses proposed SNN models and other existing methods within this rapidly advancing field of Neuromorphic Engineering~(NE), we propose that a large dataset of spike-based visual stimuli is needed and a corresponding evaluation methodology is also required to \protect\TLSdel{overall} estimate the \protect\TLSins{overall} performance of SNN models and their hardware implementations.


In this chapter we first propose an initial NE dataset based on standard computer vision benchmarks and that uses digits from the MNIST database.
This dataset is compatible with the state of current research on spike-based image recognition.
The corresponding spike trains are produced using a range of techniques: rate-based Poisson spike generation, rank order encoding, and recorded output from a silicon retina with both flashing and oscillating input stimuli.
In addition, a complementary evaluation methodology is presented to assess both model-level and hardware-level performance.
Finally, we demonstrate the use of the dataset and the evaluation methodology using an SNN model to validate the performance of the models and their hardware implementations.

With this dataset we hope to (1) promote meaningful comparison between algorithms in the field of neural computation, (2) allow comparison with conventional image recognition methods, (3) provide an assessment of the state of the art in spike-based visual recognition, and (4) help researchers identify future directions and advance the field.

In Section~\ref{sec:chapt6_intro}, diverse neuromorphic vision tasks are introduced to illustrate the requirements for a unified spike-based dataset.
The rest of this chapter is structured as follows: Section~\ref{sec:method} elaborates the purpose and protocols of the proposed dataset and describes the sub-datasets and the methods employed to generate them; it also demonstrates the suggested evaluation methodology for use with the dataset.
Section~\ref{sec:test} presents an SNN case study as demonstrations of using the dataset to assess model performance and benchmark hardware platforms.
Finally, Section~\ref{sec:summ} summarises the chapter.

\section{Introduction}
\label{sec:chapt6_intro}
Researchers are using the capabilities created by rapid developments in neuromorphic engineering to address the dual aims of understanding brain functions and building brain-like machines~\citep{furber2007neural}.
Neuromorphic engineering has delivered biologically-inspired sensors such as DVS~(Dynamic Vision Sensor) silicon retinas~\citep{serrano2013128, delbruck2008frame, yang2015dynamic, posch2014retinomorphic}, which offer the prospect of low-cost visual processing thanks to their event-driven and redundancy-reducing style of information representation.
Moreover, SNN simulation tools~\citep{davison2008pynn, gewaltig2007nest, goodman2008brian} and neuromorphic hardware platforms~\citep{furber2014spinnaker,  schemmel2010wafer,benjamin2014neurogrid,merolla2014million} have been developed to allow exploration of the brain by mimicking its functions and developing large-scale practical applications~\citep{eliasmith2012large}.
Achieving the brain's energy efficiency motivates the development of neuromorphic hardware, since the human brain consumes only about 20~W of power~\citep{drubach2000brain}.
In the case of visual processing, the brain can accurately recognise objects remarkably quickly, e.g. in 200~ms in monkeys~\citep{fabre1998rapid}, even with short presentations (less than 100~ms) of the target objects~\citep{keysers2001speed}.
Such rapid and highly accurate recognition is the target of modelling spike-based visual recognition.



Inspired by biological studies of the visual ventral pathway, SNN models have successfully been adapted to visual recognition.  \citet{riesenhuber1999hierarchical} proposed a quantitative modelling framework for object recognition with position-, scale- and view-invariance.
Their cortex-like model has been analysed on several datasets~\citep{serre2007robust}.
\protect\TLSdel{Recently~}
\protect\TLSins{Recently~}\citet{fu2012spiking} reported that their SNN implementation was capable of recognising facial expressions with a classification accuracy (CA) of 97.35\% on the JAFFE dataset~\citep{lyons1998coding} which contains 213 images of 7 facial expressions posed by 10 individuals.
According \protect\TLSdel{to~,} \protect\TLSins{to~}\citet{vanrullen2002surfing}\protect\TLSins{,} the first wave of spikes carry explicit information through the ventral stream and in each stage meaningful information is extracted and spikes are regenerated. 
Using one spike per neuron, similar to the first spiking wave in \protect\TLSdel{biology,~} \protect\TLSins{biology,~}\citet{delorme2001face} reported 100\% and 97.5\% accuracies on the face identification task over
training (40 individuals $\times$ 8 images) and testing data (40 individuals $\times$ 2 images).

Convolutional Neural \protect\TLSdel{Network (ConvNet),} \protect\TLSins{Networks (ConvNets),} inspired from the receptive field of vision cortex, \protect\TLSdel{has} \protect\TLSins{have} been implemented on spiking neurons.
An early Spiking ConvNet model identified the faces of 35 persons with a CA of 98.3\% exploiting simple integrate and fire neurons~\citep{matsugu2002convolutional}.
Another Spiking ConvNet model~\citep{zhao2014feedforward} was trained and tested both with DVS raw data and Leaky Integrate-and-Fire (LIF) neurons.
It was capable of recognising three moving postures with a CA of about 99.48\% and classifying hand-written digits with 88.14\% accuracy on the MNIST-DVS dataset (see Section~\ref{sec:data}).
In a further step \protect\TLSdel{forward,~} \protect\TLSins{forward,~}\citet{camunas2012event} implemented a convolution processor module in hardware which could be combined with a DVS for high-speed recognition tasks.
The inputs of the ConvNet were continuous spike events instead of static images or frame-based videos. 
The chip was capable of detecting the four suits in a 52-card deck which was browsed rapidly in only 410 ms.
Similarly, a real-time gesture recognition model~\citep{liu2014real} was implemented on a neuromorphic system with a DVS as a front-end and a SpiNNaker~\citep{furber2014spinnaker} machine as the back-end, where LIF neurons built up the ConvNet configured with biological parameters.
In this study's largest configuration, a network of 74,210 neurons and 15,216,512 synapses used 290 SpiNNaker cores in parallel and reached 93.0\% accuracy. 

Spike-Timing-Dependent Plasticity (STDP) as  a learning mechanism based on biological observations has been applied to vision tasks.

\citet{bichler2012extraction} demonstrated an unsupervised STDP learning model to classify car trajectories captured with a DVS retina. 
A similar model trained with STDP on winner-take-all~(WTA) circuit was tested on a Poissonian spike presentation of the MNIST dataset achieving a performance of 95.0\%~\citep{diehl2015unsupervised}.
Another STDP trained model tested on computer simulation reached a 93.3\% CA on MNIST and had the potential to be implemented using memristors~\citep{bill2014compound}. 
Adding synaptic internal state enabled STDP to work on \protect\TLSdel{bistatble} \protect\TLSins{bistable} synapses, and this method achieved the best performance of 96.5\% among similar network architectures. 

Deep Neural Networks (DNNs) have exceeded human-level performance on image classification tasks~\citep{he2015delving}, but mainstream DNN research is focussed on continuous rather than spiking neural networks.
The spiking deep network has great potential to combine remarkable performance with energy-efficient training and operation.
Early research into spiking deep networks focussed on converting off-line trained deep network into SNNs~\citep{o2013real}.
The network was initially implemented on an FPGA and achieved a CA of 92.0\%~\citep{neil2014minitaur}, while a later implementation on SpiNNaker scored 95.0\%~\citep{Stromatias2015scalable}.
Recent advances have contributed to better translation by using modified units in a ConvNet~\citep{cao2015spiking} and tuning the weights and thresholds~\citep{diehl2015fast}.
The latter paper claims a state-of-the-art performance (99.1\% on the MNIST dataset) compared to the original ConvNet.
The current trend towards training Spiking DNNs on line using biologically-plausible learning methods is also promising.
An event-driven Contrastive Divergence (CD) training algorithm for Restricted Boltzmann Machines (RBMs) was proposed for Deep Belief Networks (DBNs) using LIF neurons with STDP synapses and verified on MNIST with a CA of 91.9\%~\citep{neftci2013event}.
The work extended to use stochastic synapses, and the recognition performance increased to 95.8\%~\citep{neftci2016stochastic}.


Despite the promising research on SNN-based vision recognition, there is no commonly used database in the format of spike stimuli.
In the studies listed above, all of the vision data used are in one of the following formats:
(1) raw grey-scale images data;
(2) pixel-intensity-driven rate-based Poisson spike trains;
(3) unpublished spike-based videos recorded from DVS silicon retinas.
However, in the field of conventional non-spiking computer vision, there are a number of datasets playing important roles at different times and with various objectives~\citep{lecun1998gradient,deng2009imagenet,blank2005actions,liu2009recognizing}.
In consequence, a new set of spike-based vision datasets is now needed to quantitatively measure progress within the rapidly advancing field of spike-based visual recognition and to provide resources to support fair competition between researchers.

Apart from using spikes instead of the frame-based data used in conventional computer vision, new concerns arise when evaluating neuromorphic vision, such as latency and energy consumption, in addition to recognition accuracy.
These concerns naturally derive from the goal of spike-based visual recognition: mimicking the fast recognition with low-energy processing in the brain. 
Therefore a set of common metrics for performance evaluation in spike-based vision is required to assess SNN models and their hardware implementations.
In this chapter we propose a large dataset of spike-based visual stimuli and a complementary evaluation methodology.
Just as research in this field is an expanding and evolving activity, the dataset will be adapted and extended to fit new requirements presented by advances in the field.





\section{Materials and Methods}
\label{sec:method}
\subsection{Guiding Principles}
The NE database we propose here is a developing and evolving dataset consisting of various spike-based representations of images and videos.
The spikes are either generated from spike encoding methods which convert images or frames of videos into spike trains, or recorded from DVS silicon retinas.
The spike trains are in the format of Address-Event Representation~(AER)~\citep{mahowald1992vlsi} data, which are suitable for both event-driven computer simulations and neuromorphic systems.
AER was originally proposed as a time-multiplexed spike communication protocol where each time a neuron produces a spike an event is generated that codes the spiking neuron's address on a fast time-multiplexed digital bus.
The recorded AER data consists of a list of events, each one containing the time stamp of a spike and the address of the neuron which generated the spike.
With the NE dataset we hope:
\begin{itemize}
	\item \textit{to promote meaningful comparisons of algorithms in the field of spiking neural computation.}
	The NE dataset provides a unified format of AER data to meet the demands of spike-based visual stimuli.
	It also encourages researchers to publish and contribute their data to build up the NE dataset.
	\item \textit{to allow comparison with conventional image recognition methods.}
	We expect the dataset to support this comparison using spiking versions of existing vision datasets.
	Thus, conversion methods are required to transform datasets of images and frame-based videos into spike stimuli.
	More biologically-accurate and better information preserving schemes are welcome.
	
	\item \textit{to provide an assessment of the state of the art in spike-based visual recognition on neuromorphic hardware.}
	To reveal the accuracy, speed, and energy-efficient recognition of neuromorphic approaches, we need not only a spike-based dataset but also an appropriate evaluation methodology.
	The evaluation methodology will be constantly improving along with the evolution of the dataset.
	\item \textit{to help researchers identify future directions and advance the field.}
	The development of the dataset and its evaluation methodology will introduce new challenges for the neuromorphic engineering community.
	However, these must represent an appropriate degree of difficulty: a too-easily-solved problem turns into a tuning competition, while a problem that is too difficult will not yield meaningful assessment.
	So suitable problems should continuously be added to promote future research.  
\end{itemize}


\subsection{The Dataset: NE15-MNIST}
\label{sec:data}
The first proposed dataset in the benchmarking system is NE15-MNIST (Neuromorphic Engineering 2015 on MNIST).
NE15-MNIST is the spiking version of an original non-spiking dataset which was downloaded from the MNIST Database of Handwritten Digits~\citep{lecun1998gradient}  \protect\TLSdel{website\footnote{http://yann.lecun.com/exdb/mnist/}.}  \protect\TLSins{website\footnote{\url{http://yann.lecun.com/exdb/mnist/}}.}
Due to its straightforward target of classifying real-world images, the plain format of the binary data and simple patterns, MNIST has been one of the most popular datasets in computer vision for over 20 years.
MNIST is a popular task among the neuromorphic vision research \protect\TLSdel{community as stated in Section~\ref{sec:intro}.} \protect\TLSins{community.}
The converted MNIST dataset consists of four subsets which were generated for different purposes:
\begin{itemize}
	\item \textit{Poissonian},
	which encodes each pixel as a Poisson spike train and is intended for benchmarking existing rate-based SNN models.
	\item \textit{FoCal~(Filter Overlap Correction ALgorithm)},
	to promote the study of spatio-temporal algorithms applied to recognition tasks using small numbers of input spikes.
	\item \textit{DVS recorded flashing input},
	to encourage research into fast recognition methods to mimic the rapid and accurate `core recognition' in the primate ventral visual pathway~\citep{dicarlo2012does}.
	\item \textit{DVS recorded moving input},
	to trigger the study of algorithms targeting continuous input from real-world sensors for implementation, for example, on mobile neuromorphic robots.
\end{itemize}
The dataset \protect\TLSdel{can be found} \protect\TLSins{is published} in the \protect\TLSdel{GitHub repository at: https://github.com/NEvision/NE15.} \protect\TLSins{GitHub: \url{https://github.com/NEvision/NE15}.}
\subsection{Data Description}
Two file formats are supported in the dataset: the jAER format~\citep{delbruck2008frame} (.dat or .aedat), and binary files in NumPy~\citep{numpyPython} (.npy) format.
The spikes in jAER format, whether recorded from a DVS retina or artificially generated, can be displayed by the jAER software.
Figure~\ref{fig:zero}(a) is a snapshot of the software displaying a .aedat file which was recorded from a DVS retina~\citep{serrano2013128}.
The resolution of the DVS recorded data is 128$\times$128.
The second spike-based format used is a list of spike source arrays in PyNN~\citep{davison2008pynn}, a description language for building spiking neuronal network models.
Python code is provided for converting from either file format to the other.
The duration of the artificially-generated data can be configured using the Python code provided, while the recorded data varies in duration: 1~s for the flashing input, and 3.2 to 3.4~s for the moving input.




\begin{figure}[bht!]
	\centering
	\includegraphics[width=0.8\textwidth]{pics_bench/fig1.jpg}
	\caption{
		Snapshots of the jAER software displaying spike-encoded videos.
		The same image of digit `0' is transformed into spikes by (a) DVS recording and (b) Poisson generation.
		(c) A raster plot of the Poisson spike trains.}
	\label{fig:zero}
\end{figure}

\subsubsection{Poissonian}
\label{sec:poissonian}
The timing of spikes in the cortex is highly irregular~\citep{squire1998findings}.
An interpretation is that the inter-spike interval reflects a random process driven by the instantaneous firing rate.
If the generation of each spike is assumed to be independent of all other spikes, the spike train is seen as a Poisson process.
The spike rate can be estimated by averaging the pooled responses of the neurons.


As stated above, rate coding is generally used in presenting images as spike trains.
The spike rate of each neuron accords with the intensity of the corresponding pixel.
Instead of providing exact spike arrays, we share the Python code for generating the spikes.
Each recognition system may require different spike rates and durations.
The generated Poisson spike trains can be in both jAER and PyNN spike source array formats.
Thus, it is easy to visualise the digits and also to couple the spike trains into spiking neural networks.
Because different simulators generate random Poisson spike trains with different mechanisms, languages and codes, using the same dataset enables performance evaluation on different simulators without the confusion created by differences in input.
The same digit displayed in Figure~\ref{fig:zero}(a) can be converted into Poisson spike trains, see Figure~\ref{fig:zero}(b).
A raster plot of the Poisson spike trains is shown in Figure~\ref{fig:zero}(c).



\subsubsection{Rank Order Encoding}
A different way \protect\TLSdel{ofencoding} \protect\TLSins{of encoding} spikes is to use a rank order code; this means
keeping just the order in which the spikes fired and disregarding their exact timing.
Rank-ordered spike trains have been used in vision tasks under a biological plausibility constraint, making them a viable way of encoding images for neural applications~\citep{van2001rate,sen2009evaluating,masmoudi2010novel}.
Rank order coding (ROC) can be performed using an algorithm known as the
{FoCal algorithm~\citep{sen2009evaluating}}.
This algorithm models the foveola, the highest resolution area of the retina, with four ganglion cell layers each with a different scale of centre-surround receptive field~\citep{kolb2003retina}.
The detailed description of the algorithm is demonstrated \protect\TLSdel{in~} by \protect\TLSdel{Garibaldi~Pineda-Garc\'ia\,} \protect\TLSins{Garibaldi~Pineda-Garc\'ia\,~}\citep{liu2016bench} and the source Python scripts to transform images to ROC spike trains, and to convert the results into AER and PyNN spike source arrays, can be found in the dataset website.


\subsubsection{DVS Sensor Output with Flashing Input}
\label{subsec_flash}
The purpose of including the subset with DVS-recorded flashing digits is to promote research into rapid and accurate `core recognition', thus to encourage applying non-rate-based algorithms, for example ROC, to short DVS output spike trains.

Each digit was shown alternating with a blank image and each display lasted one second.
The digits were displayed on an LCD monitor in front of the DVS retina~\citep{serrano2013128} and were placed in the centre of the visual field of the camera.
Since there are two spike polarities - `ON' indicating an increase in the intensity while `OFF' indicates a decrease - there are `ON' and `OFF' flashing recordings respectively per digit.
In Figure~\ref{fig:flash}, the burstiness of the spikes is illustrated where most of the spikes occur in a 30~ms time slot. 
In total, this subset of the database contains \protect\TLSdel{2$\times$$60,000$} \protect\TLSins{2$\times$60K} recordings for training and \protect\TLSdel{2$\times$$10,000$} \protect\TLSins{2$\times$10K} for testing.

\begin{figure}[tbh!]
	\centering
	\includegraphics[width=0.8\textwidth]{pics_bench/fig5.jpg}	
	\caption{DVS sensor with flashing input.
		Blue is used for `ON' events and green for `OFF' events.
		(a) The raster plot shows spikes generated by individual neurons over time.
		It is hard to recognise the total number of spikes due to the large number of neurons involved in the figure.
		Thus all the spikes are ordered in time, and displayed in the figure below.
		(b) The raster plot shows \protect\TLSins{the} ordered spike sequence over time.
		The total number of spikes \protect\TLSdel{are} \protect\TLSins{is} around \protect\TLSdel{$7,000$} \protect\TLSins{7K} for both `ON' and `OFF' events.
		The bursty nature of the resulting spikes is illustrated, where most of the spikes occur in a 30~ms time slot.}
	\label{fig:flash}
\end{figure}

\subsubsection{DVS Sensor Output with Moving Input}

The subset with DVS recorded moving digits is provided by a collaborator, Teresa~Serrano-Gotarredona, and is  presented to address the challenges of position- and scale- invariance in computer vision.

MNIST digits were scaled to three different sizes, using smooth interpolation algorithms to increase their size from the original 28x28 pixels, and displayed on the monitor with slow motion. 
The same DVS~\citep{serrano2013128} used in Section~\ref{subsec_flash} captured the movements of the digits and generated spike trains for each pixel in its 128$\times$128 resolution.
A total of \protect\TLSdel{$30,000$} \protect\TLSins{30K} recordings were made: 10 digits, at 3 different scales, \protect\TLSdel{$1,000$} \protect\TLSins{1K} different handwritten samples for each.

\subsection{Performance Evaluation}
\label{sec:eval}
As a result of the spike-based processing used in SNN models, new concerns about the latency and energy cost arise over performance assessment.
Therefore we propose corresponding evaluation metrics and suggest a sufficient description of SNN models in this section.
Once a model is implemented on a neuromorphic platform, the hardware performance can be evaluated by running the particular model.
This model-specific assessment provides more robust comparisons between hardware platforms by using the same network topology, neuron and synaptic models, and learning rules. 
A complementary evaluation methodology is essential to provide common metrics and assess both the model-level and hardware-level performance.


\subsubsection{Model-Level Evaluation}
\label{subsec:model}

A suggested description of an SNN model is shown in Table~\ref{tb:model_eval} where the performance evaluation metrics are in bold and the SNN specific description is in italics.

Because SNNs introduce the time dimension and spike-based processing, additional performance metrics become relevant in addition to classification accuracy: recognition latency and the number of synaptic events.
Recognition latency measures how fast spikes are conveyed through the layers of network to trigger the recognition neurons.
\protect\TLSdel{ considers}
\citet{dicarlo2012does}\protect\TLSins{ considered} the rapid ($<$200~ms) and accurate vision recognition in the brain as the essential problem of object recognition.
For real-time systems with live visual inputs, such as robotic systems, a short response latency helps make fast decisions and take rapid action.
The latency is measured as the time difference between the first spike generated by the output layer and the first spike from the input layer.
A small number of total synaptic events generated by a recognition task indicates the efficiency of the SNN model.
A spike event is a synaptic operation evoked when one action potential is transmitted through one synapse~\citep{sharp2012power}.
Fewer spike events imply lower overall neural activity and lower energy consumption.
The number of synaptic events can be measured as `Sopbs', synaptic operations per biological second.

\begin{table*}[hbt!]
	\caption{SNN descriptions at the model level}
	\begin{center}
		\bgroup
		\def\arraystretch{1.5}
		\begin{tabular}{ l l}
\hline
			\begin{rightcell}{3cm}Input\end{rightcell} & 
			\begin{leftcell}{5cm} 
				\textit{converting methods}\\
				preprocessing 
			\end{leftcell} \\
\begin{rightcell}{3cm} Network\end{rightcell} &
			\begin{leftcell}{5cm}
				topology\\
				\textit{neuron and synaptic type}
			\end{leftcell}\\  \begin{rightcell}{3cm} Training \end{rightcell} & 
			\begin{leftcell}{5cm}
				supervised or not\\
				\textit{learning rule} \\ 
				\textit{biological training time}
			\end{leftcell}\\  \begin{rightcell}{3cm} Recognition \end{rightcell} &
			\begin{leftcell}{5cm} 
				\textbf{classification accuracy}\\ 
				\textbf{\textit{response latency}}\\
				\textbf{\textit{number of synaptic events}} \\
				\textit{biological testing time}\\
				\textit{input spiking rate}
			\end{leftcell}\\% recognition
			\hline
		\end{tabular}
		\egroup
	\end{center}
	\label{tb:model_eval}
\end{table*}


Alongside the SNN evaluation metrics, a sufficient description of a network model is required so that other researchers can reproduce it and compare it with other models.
First of all, the input of an SNN model is specified.
The description includes the transformation method for converting raw images to spike trains, and the preprocessing either to images or spikes.
Filtering the raw image may ease the classification/recognition task while adding noise may require more robustness in the model.
Secondly, as with the evaluation of conventional artificial neural networks, a description of the network characteristics provides the basis for the overall performance evaluation.
Sharing the designs not only makes the model reproducible but also inspires fellow scientists to bring new points of view to the problem, generating a positive feedback loop where everybody wins.
The main characteristics include the network topology and the neural and synaptic models.
The network topology defines the number of neurons used for each layer and the connections between layers and neurons.
It is essential to state the types of neural and synaptic model (e.g. current-based LIF neuron) utilised in the network and the parameters configuring them, because neural activities differ significantly between configurations.
Any non-neural classifier, sometimes added to aid the design or enhance the output of the network, must also be specified.
Thirdly, the training procedure determines the recognition capability of a network model.
Specifying the learning algorithm with its mechanism (supervised, semi-supervised and unsupervised) helps the reader understand the core features of the model.
A detailed description of new spike-based learning rules will be a great contribution to the field due to the present paucity of spatio-temporal learning algorithms.
Most publications reflect the use of adaptations to existing learning rules; details on these modifications should be clear and unambiguous.
In conventional computer vision, the number of iterations of training images presented to the network play an important role.
Similarly, the biological training time determines the amount of information provided for training an SNN.
Finally in the testing phase, as well as the performance evaluation metrics stated above, specific configurations of the input spikes are also essential.
This includes details of the way samples are presented to the network: spiking rates, and biological time per test sample.
The combination of these two factors determines how much information is presented to the network.
Following  the formatted evaluation as in Table~\ref{tb:model_eval}, Table~\ref{tb:software_comparison} lists a few SNN models  for MNIST classification, although some details are missing. 

\begin{table}[htbp] \small 
	\caption{Model-level comparison}
	\begin{center}
\def\arraystretch{1}
		\begin{tabular}{ l c c c c }
			$ $ &
			\begin{mycell}{2cm} Input\end{mycell} & 
			\begin{mycell}{3cm} Network\end{mycell} & 
			\begin{mycell}{2.5cm} Training \end{mycell} & 
			\begin{mycell}{3cm} Recognition \end{mycell} \\
			\hline
			
			\begin{mycell}{0.5cm}~\citep{brader2007learning} \end{mycell} & 
			\begin{mycell}{2cm} Poisson \end{mycell} & \begin{mycell}{3cm} Two layer,\\LIF neurons\\bistable synapse\end{mycell}&  \begin{mycell}{3cm} Semi-supervised, STDP, calcium LTP/LTD\end{mycell}&  \begin{mycell}{3cm} 96.5\% \end{mycell} \\% recognition

			\begin{mycell}{0.5cm}~\citep{diehl2015unsupervised} \end{mycell} & 
			\centering Poisson&
			\begin{mycell}{3cm} Two layers, LIF neurons, inhibitory feedback  \end{mycell}& 
			\begin{mycell}{3cm} Unsupervised, WTA, STDP,\\  200K~s times\\ 15 iterations\end{mycell} & 
			\begin{mycell}{3cm} 95\% \end{mycell}\\
			
			\begin{mycell}{0.5cm}~\citep{neftci2013event} \end{mycell} & 
			\begin{mycell}{2cm} Thresholding, Poisson\end{mycell} & \begin{mycell}{3cm} Two layers, \\RBM, \\ LIF neurons \end{mycell}&  \begin{mycell}{3cm} Event-driven contrastive divergence (eCD), unsupervised \end{mycell}&  \begin{mycell}{3cm} 91.9\% \\ 1~s per test\end{mycell} \\% recognition
			
			\begin{mycell}{0.5cm}~\citep{neftci2016stochastic} \end{mycell} & 
			\begin{mycell}{2cm} Thresholding, Poisson\end{mycell} & \begin{mycell}{3cm} Two layers,\\RBM, \\ LIF neurons \end{mycell}&  \begin{mycell}{3cm} Synaptic Sampling Machine + eCD, unsupervised \end{mycell}&  \begin{mycell}{3cm} 95.8\% \\ 250~ms per test\end{mycell} \\% recognition
			\begin{mycell}{0.5cm} \citep{stromatias2015robustness} \end{mycell} & 
			\begin{mycell}{2cm} Poisson \end{mycell} & \begin{mycell}{3cm} Four layers, \\RBM, \\ LIF neurons \end{mycell}&  \begin{mycell}{3cm} Off-line trained, unsupervised \end{mycell}&  \begin{mycell}{3cm} 94.94\%\\16 ms latency \\ 1.44M Sopbs\end{mycell} \\% recognition
			
			\begin{mycell}{0.5cm}~\citep{diehl2015fast}\end{mycell}& 
			\begin{mycell}{2cm}Poisson\end{mycell} & \begin{mycell}{3cm} Six layers,\\ConvNet, \\IF neurons\end{mycell}& \begin{mycell}{3cm} Off-line trained with ReLU, weight normalisation \end{mycell}&   \begin{mycell}{3cm} 99.1\%, \\0.5~s per test\end{mycell}\\ \begin{mycell}{0.5cm}~\citep{zhao2014feedforward}\end{mycell}  & 
			\begin{mycell}{2cm} Thresholding \\ or DVS \end{mycell}& \begin{mycell}{3cm} Simple (Gabor), \\Complex (MAX) \\and Tempotron  \end{mycell}& \begin{mycell}{3cm} Tempotron, supervised \end{mycell}& \begin{mycell}{3cm} 91.3\%(Thresholding)\\ 11~s per test \\ 88.1\%(DVS),\\ 2~s per test\end{mycell}\\ 
			\begin{mycell}{0.5cm}Chpt4\end{mycell}& 
			\begin{mycell}{2cm}Poisson\end{mycell} & \begin{mycell}{3cm} Six layers,\\ConvNet, \\LIF neurons\end{mycell}& \begin{mycell}{3cm} Off-line trained with ReLU,\\ Noisy Softplus fine-tune\end{mycell}&   \begin{mycell}{3cm} 99.07\%, \\1~s per test\\$53.4$M Sopbs\end{mycell}\\ 
			\begin{mycell}{0.5cm}Chpt5\end{mycell}& 
			\begin{mycell}{2cm}Poisson\end{mycell} & \begin{mycell}{3cm} Two layers,\\ Autoencoders~(AE), \\LIF neurons\end{mycell}& \begin{mycell}{3cm} Event-driven, spike-based  AE\\18K~s training, unsupervised\end{mycell}&   \begin{mycell}{3cm} 94.72\%, \\1~s per test\\$5.26$M Sopbs\end{mycell}\\ \begin{mycell}{0.5cm} Case Study \end{mycell}  & 
			\begin{mycell}{2cm} Poisson \end{mycell}& \begin{mycell}{3cm} Fully connected decision layer, \\ LIF neurons \end{mycell}& \begin{mycell}{3cm} K-means clusters,\\Supervised  STDP\\18K~s of training \end{mycell}& \begin{mycell}{3cm} 92.99\%\\1~s per test \\13.82~ms latency\\$4.17$M Sopbs\end{mycell}\\ \end{tabular}
\end{center}
	\label{tb:software_comparison}
\end{table}

\subsubsection{Hardware-Level Evaluation}
\label{subsec:hw}

\begin{sidewaystable*}[htbp] \small
	\caption{Hardware-level comparison}
	\begin{center}
		\begin{minipage}{\textwidth}
			
			\begin{savenotes}
				\bgroup
				\def\arraystretch{1.4}
				\begin{tabular}{l c c c c c c}
					$ $ & 
					\begin{mycell}{2.0cm} System \end{mycell} & 
					
					\begin{mycell}{4.0cm} Neuron Model \end{mycell} & 
					\begin{mycell}{2.0cm}Synaptic\\Plasticity\end{mycell} &
					\begin{mycell}{2.0cm} Precision \end{mycell} &  
					\begin{mycell}{2.0cm} Simulation\\Time \end{mycell} & 
					\begin{mycell}{2.0cm} Energy \\Usage \end{mycell} 
					\\
					\hline
					
					\begin{mycell}{1.8cm} SpiNNaker \citep{stromatias2013power} \end{mycell} &
					\begin{mycell}{2.0cm} Digital, \\Scalable \end{mycell} & 
					\begin{mycell}{4.0cm}Programmable\\Neuron and Synapse,\\Axonal delay \end{mycell}& 
					\begin{mycell}{2.0cm}Programmable\\learning rule\end{mycell}& 
					\begin{mycell}{2.0cm}11- to 14-bit synapses\end{mycell} & 
					\begin{mycell}{2.0cm} Real-time \\ Flexible time resolution \end{mycell}  &
					\begin{mycell}{2.5cm} 8~nJ/SE \end{mycell} \\
					\begin{mycell}{1.8cm} TrueNorth \citep{merolla2014million}\end{mycell} & \begin{mycell}{2.0cm}Digital, \\Scalable \end{mycell}& 
					\begin{mycell}{4.0cm}Fixed models,\\Config params,\\Axonal delay\end{mycell}& 
					\begin{mycell}{2.0cm}No plasticity\end{mycell}& 
					\begin{mycell}{2.2cm}122 bits \\params \& states,
						\\4-bit/\\4 values\\synapses 
						\footnote[1]{We consider them 4-bit synapses because it is only possible to choose between 4 different signed integers and whether the synapse is active or not.}
					\end{mycell}& 
					\begin{mycell}{2.0cm}Real-time\end{mycell}& 
					\begin{mycell}{2.0cm}26 pJ/SE\end{mycell} \\
					
					\begin{mycell}{1.8cm} Neurogrid \citep{benjamin2014neurogrid}\end{mycell} &
					\begin{mycell}{2.0cm}Mixed-mode,\\Scalable\end{mycell} & 
					\begin{mycell}{4.0cm}Fixed models,\\Config params\end{mycell} & 
					\begin{mycell}{2.0cm}Fixed rule\end{mycell} & 
					\begin{mycell}{2.0cm}13-bit shared \\ synapses\end{mycell} &
					\begin{mycell}{2.0cm}Real-time\end{mycell} &
					\begin{mycell}{2.0cm}941 pJ/SE\end{mycell} \\
					\begin{mycell}{1.8cm} HI-CANN \citep{schemmel2010wafer}  \end{mycell} & \begin{mycell}{2.0cm}Mixed-mode,\\Scalable\end{mycell} &
					\begin{mycell}{4.0cm}Fixed models,\\Config params\end{mycell}& 
					\begin{mycell}{2.0cm}Fixed rule\end{mycell}& 
					\begin{mycell}{2.0cm}4-bit/\\16 values\\synapses\end{mycell}& 
					\begin{mycell}{2.0cm}Faster than\\ real-time
						\footnote[2]{A maximum speed-up of up to $10^5$ times real time has been reported.}
					\end{mycell}&
					\begin{mycell}{2.0cm} 7.41 nJ/SE\\(network only) \end{mycell}\\
					\begin{mycell}{1.8cm} HiAER-IFAT \citep{yu201265k}\end{mycell} & 
					\begin{mycell}{2.0cm}Mixed-mode,\\Scalable\end{mycell} &
					\begin{mycell}{4.0cm}Fixed models,\\Config params\end{mycell}& 
					\begin{mycell}{2.0cm}No plasticity\end{mycell} &  
					\begin{mycell}{2.0cm}Analogue neuron/synapse\end{mycell} & 
					Real-time&
					\begin{mycell}{2.0cm}22-pJ/SE\\\citep{park201465k}\end{mycell}
					
					\end{tabular}
				\egroup
				
			\end{savenotes}
		\end{minipage}
	\end{center}
	\label{tb:hardware_comparison}
\end{sidewaystable*}

A direct comparison between neuromorphic platforms is a non-trivial task due to the different hardware implementation technologies as mentioned in Section~\ref{subsec:neuromorphic_hw}.
Table~\ref{tb:hardware_comparison} attempts to describe the neuromorphic hardware platforms with reference to different aspects of SNN simulation.
The scalability of a hardware platform determines the network size limit of a neural application running on it.
Considering the various neural and synaptic models, plasticity learning rules and lengths of axonal delays, a programmable platform offers flexibility to support diverse SNNs while a hard-wired system supporting only specific models is advantageous due to its energy-efficiency and simpler design and implementation.
The classification accuracy of an SNN running on a hardware system can be different from the software simulation, since hardware implementations may impose limits on the precision used for the membrane potentials of neurons (for the digital platforms) and the synaptic weights.
Simulation time is another important measure when running large-scale networks on hardware.
Real-time implementation is an essential requirement for robotic systems because of the real-time input from the neuromorphic sensors.
Running faster than real time is attractive for large and long simulations.
It is interesting to compare the performance of each platform in terms of energy requirements, especially if the platform targets mobile applications and robotics.
Some researchers have suggested the use of energy per synaptic event (J/SE)~\citep{sharp2012power,stromatias2013power} as an energy metric because the large fan in and out of a neuron means that synaptic processing tends to dominate the total energy dissipation during a simulation.

\citet{merolla2014million} proposed the number of synaptic operations per second per Watt (Sops/W).
These two measures are equivalent, since J/SE$\times$Sops/W = 1.

However, the typical reported simulation time and energy use for the various platforms is under different SNN models, making the comparisons problematic.
Model-specific hardware metrics would provide robust comparisons between platforms and expose how different networks influence the metrics on particular hardware.
The proposed evaluation metrics consist of the \textbf{feasibility}, \textbf{classification accuracy}, \textbf{simulation time} and \textbf{energy use}.
A particular SNN model is feasible to run on a particular hardware platform only when the network size is under the platform's limit, the neural and synaptic models are supported, and the learning rule is implemented.
CA also plays a role in hardware evaluation because of the precision limits that may be imposed by the platform.
Due to the limited hardware resources, simulation time may accelerate or slow down according to the network topology and spike dynamics.
Similarly, energy costs vary with different networks and neural and synaptic models.

\section{Results}
\label{sec:test}
In this section, we present a recognition SNN model working on the Poissonian subset of the NE15-MNIST dataset.
The network components, training and testing methods are described along the lines set out in Section~\ref{subsec:model}.
The recognition result is evaluated using the proposed metrics: classification accuracy, response latency and number of synaptic events.
As tentative benchmarks the models are implemented on SpiNNaker to assess the hardware-level performance against software simulators.
Presenting proper benchmarks for vision recognition systems is still under investigation; the case study only make a first attempt.

The case study is a simple two-layer network where the input neurons receive Poisson spike trains from the dataset and form a fully connected network with the decision neurons.
There is at least one decision neuron per digit to classify a test input.
The neuron with the highest output firing rate classifies a test image as the digit it represents.
The model utilises LIF neurons, and the parameters are all biologically valid, see the listed values in Table~\ref{tbl:pynnSetting}.
The LIF neuron model follows the membrane potential dynamics:

\begin{equation}
\tau_m \frac{\D V}{\D t}=V_{rest} - V + R_{m} I_{syn}(t) ~~~,
\label{eq:LIF}
\end{equation}

where $\tau_m$ is the membrane time constant, $ V_{rest} $ is the resting potential, $ R_{m} $ is the membrane resistance and $ I_{syn} $ is the synaptic input current.
In PyNN, $ R_{m} $ is  represented by $ R_{m}=\tau_m/C_{m} $, where $C_{m} $ is the membrane capacitance.
A spike is generated when the membrane potential goes beyond the threshold, $ V_{thresh} $ and the membrane potential then resets to $V_{reset}$.
In addition, a neuron cannot fire within the refractory period, $ \tau_{refrac} $, after generating a spike.

The connections between the input neurons and the decision neurons are plastic, so the connection weights can be modulated during training with a multiplicative STDP learning rule,  refer to Section~\ref{subsec:STDP} for more detail.
The model is described with PyNN and the code is published in
the Github repository with the dataset.
As a potential benchmark, this system is composed of simple neural models, trained with standard learning rules and written in a standard SNN description language. These characteristics allow the same network to be tested on various simulators, both software- and hardware-based.

Both training and testing use the Poissonian subset of the NE15-MNIST dataset.
This makes performance evaluation on different simulators possible with the unified spike source array provided by the dataset. 
In terms of this case study, the performance of the model was evaluated with both software simulation (on NEST~\citep{gewaltig2007nest}) and on a hardware implementation (on SpiNNaker).

In order to fully assess the performance, different settings were configured on the network, such as network size, input rate and test image duration.
For simplicity of describing the system, one standard configuration is set as the example in the following sections.

\begin{table}[hbbp]
	\centering
	\caption{\label{tbl:pynnSetting}Parameter setting for the current-based LIF neurons using PyNN.}
	\bgroup
	\def\arraystretch{1.4}
	\begin{tabular}{c c c}
		Parameters & Values & Units \\
		\hline
		cm & 0.25 & nF	\\
		tau\_m & 20.0 & ms\\
		tau\_refrac & 2.0 & ms\\
		v\_reset & -70.0 & mV\\
		v\_rest & -65.0 & mV\\
		v\_thresh & -50.0 & mV\\
		\end{tabular}
	\egroup
\end{table}

\subsection{Training}
There are two layers in the model: 28$\times$28 input neurons fully connect to 100 decision neurons.
Each decision neuron responds to a certain digit template.
In the standard configuration, there are 10 decision neurons responding to each digit with slightly different templates.
Those templates are embedded in the connection weights between the two layers.
Figure~\ref{fig:model}(a) shows how the connections to a single decision neuron are tuned.

\begin{figure}[thb!]
	\centering
	\includegraphics[width=0.8\textwidth]{pics_bench/fig6.jpg}
	\caption{The (a) training and (b) testing model of the two-layered spiking neural network.}
	\label{fig:model}
\end{figure} 

The training set of \protect\TLSdel{$60,000$} \protect\TLSins{60K} hand written digits are firstly classified into 100 classes, 10 subclasses per digit, using K-means clusters.
K-means clustering separates a set of data points into K subsets (clusters) according to the Euclidean distance between them.
Therefore each cluster tends to form a boundary within which the data points are near to each other.
In this case, all the images of the same digit (a class) are divided into 10 subclasses by assigning K=10.
Then the images in a certain subclass are used to train a template embedded in the synaptic weights to the corresponding decision neuron.
The firing rates of the input neurons are assigned linearly according to their intensities and the total firing rate of all the 28$\times$28 input neurons is normalised to \protect\TLSdel{$2,000$~Hz,} \protect\TLSins{2K~Hz,} that is, the sum of the firing rates of all of the input neurons is \protect\TLSdel{$2,000$~Hz.} \protect\TLSins{2K~Hz.}
All the images together are presented for \protect\TLSdel{$18,000$~s} \protect\TLSins{18K~s} (about 300~ms per image) during training and at the same time a teaching signal of 50~Hz is conveyed to the decision neuron to trigger STDP learning.
The trained weights are plotted in accordance with the positions of the decision neurons in Figure~\ref{fig:model}(b).





\subsection{Testing}
After training the weights of the plastic synapses are set to static, keeping the state of the weights at the last moment of training.
However, during training the synaptic plasticity holds a hard limit of 0 on the weight strength, thus excitatory synapses cannot change into inhibitory.
To investigate how inhibitory connections influence the classification performance, the weak weights were set to negative with identical strengths.
Results show that inhibitory synapses significantly reduced the output firing rates while keeping a good classification ability.
Thus the strategy of replacing weak weights to \protect\TLSins{the} same negative values was used throughout the case study.
The feed-forward testing network is shown in Figure~\ref{fig:model}(b) where Poisson spike trains are generated the same way as in the training with a total firing rate of \protect\TLSdel{$2,000$~Hz} \protect\TLSins{2K~Hz} per image.
The input neurons convey the same spike trains to every decision neuron through its \protect\TLSdel{responding} \protect\TLSins{corresponding} trained synaptic weights. 
One test trial contains \protect\TLSdel{$10,000$} \protect\TLSins{10K} images in total and each image is presented once and lasts 1~s with a 0.2~s blank period between consecutive images.
The output neuron with the highest firing rate determines which digit is recognised.
With the standard training configuration, we compared the CA of different simulations of the same SNN model.
Using the trained weights from the NEST simulation, the accuracy of the recognition on NEST reached 90.03\%, and this accuracy was also achieved on SpiNNaker.
When the network was both trained and tested on SpiNNaker the recognition accuracy was 87.41\%.
Using these weights in NEST yielded a similar result (87.25\%). 
The reduction in CA using the SpiNNaker trained weights was due to precision loss caused by the limited fast memory and the necessity for fixed-point arithmetic to ensure real-time operation.
It is inevitable that numerical precision will be below IEEE double precision at various points in the processing chain from synaptic input to membrane potential.
The main bottleneck is currently in the ring buffer where the total precision for accumulated spike inputs is 16-bit, meaning that individual spikes are realistically going to be limited to 11- to 14-bit depending upon the probabilistic headroom calculated as necessary from the network configuration and spike throughput~\citep{Hopkins2015Accuracy}.

\subsection{Evaluation}
Evaluation starts from the model-level, focusing on the spike-based recognition analysis.
As mentioned in Section~\ref{subsec:model}, CA, response time (latency) and the total number of synaptic events are the main concerns when assessing the recognition performance.
In our experiment, two sets of weights were applied: the original STDP trained weights, and \protect\TLSins{the} scaled-up weights which are 10 times stronger.
The spike rates of the test samples were also modified, ranging from 10 to \protect\TLSdel{$5,000$~Hz.} \protect\TLSins{5K~Hz.}
\begin{figure}[htb!]
	\centering
	\includegraphics[width=1\textwidth]{pics_bench/fig7.jpg}
	\caption{Accuracy, response time (latency) and synaptic event rate (Sopbs) change over test time and input firing rate per test image.
		The test time is the duration of the presence of a single test image, and the input firing rate is the summation of all the input neurons.
		Original trained weights are used (up-pointing triangles with solid line) as well as the scaled up ($\times10$) weights (down-pointing triangles with dashed line). }
	\label{fig:assess}
\end{figure}

We found that accuracy depends largely on the time each sample is exposed to the network and the sample spike rate (Figure~\ref{fig:assess}).
Figure~\ref{fig:assess}(a) shows that the CA is better as exposure time increases. The longer an image is presented, the more information is gathered by the network, so the accuracy climbs.
Classification accuracy also increases when input spike rates are augmented (Figure~\ref{fig:assess}(b)).
Given that the spike trains injected into the network are more intense, the decision neurons become more active, and so does the output disparity between them.
Nonetheless, it is important to know that these increases in CA have a limit, as is shown in the aforementioned figures.
With stronger weights, the accuracy is much higher when the input firing rate is less than \protect\TLSdel{$2,000$~Hz.} \protect\TLSins{2K~Hz.}


The latency of an SNN model is the result of the input firing rates and the synaptic weights.
We measured the latency of each test by getting the time difference of the first spike generated by any decision neuron in the output layer and the first spike of the input layer.
As the input firing rates grow, there are more spikes arriving at the decision neurons, triggering them to spike sooner.
A similar idea applies to the influence of synaptic weights.
If stronger weights are taken, then the membrane potential of a neuron reaches its threshold earlier.
Figure~\ref{fig:assess}(d) indicates that the latency is shortened with increasing input firing rates with both the original and scaled-up weights.
When the spiking rate is less than \protect\TLSdel{$2,000$~Hz,} \protect\TLSins{2K~Hz,} the network with stronger weights has a much shorter latency.
As long as there are enough spikes to trigger the decision neurons to spike, increasing the test time will not make the network respond sooner~(Figure~\ref{fig:assess}(c)).

At the default configuration of the SNN model, each input neuron connects to all of the 100 decision (output) neurons with both excitatory and inhibitory projections.
Thus the synaptic events happening in the inter-layer connections are 200 times the total input firing rate.
Figure~\ref{fig:assess}(e) shows the stable Sopbs of the entire network when the input firing rate is held at \protect\TLSdel{2000~Hz} \protect\TLSins{2K~Hz} and the test time increases.
The firing rates of the output layer are relatively small, and are 0.1\% and 1.5\% of the total Sopbs using original and scaled-up weights respectively.
The variations in the total Sopbs lie in the firing rate of the output layers only, and the stronger connections lead to the higher firing rates.
Likewise, the output neurons are more active with stronger connection weights, and the gap widens as the input firing rate increases, see Figure~\ref{fig:assess}(f).
Although the variations in the Sopbs \protect\TLSdel{climbs} \protect\TLSins{climb} to around 8~kHz, it is not obvious in the figure because the output firing rates are relatively low and therefore so are the differences.


The network size not only influences the accuracy of a model but also the time taken for simulation on specific platforms, thus impacting the energy usage on the hardware.
For the purpose of comparing the accuracy, simulation time, number of synaptic events and energy usage, different configurations have been tested on NEST (working on a PC with CPU: i5-4570 and 8G \protect\TLSins{bytes} memory) and on SpiNNaker.
The same experiment was run 4~times with different random seeds; the average performance estimation is listed in Table~\ref{tbl:compare}.
The input rates in all of the tests are \protect\TLSdel{$5,000$~Hz,} \protect\TLSins{5K~Hz,} and each image is presented for 1~s with a 0.2~s blank period between consecutive images during which the model receives no input.
The configurations only differ in the number of templates (subclasses/clusters) per digit.
\begin{sidewaystable}[ph!] 
	\caption{Comparisons of NEST (N) on a PC and SpiNNaker (S) performance averaged over 10 trials.}
	\begin{center}
		\bgroup
		\def\arraystretch{1.4}
		\begin{tabular} {l| c  c c c c c}
			\multicolumn{2}{c}{Subclasses per digit} 
			& 1 & 10 & 50 & 100 & 1000 \\
			\hline
			\multicolumn{2}{c}{Avg. response}
			& \multirow{2}{*}{18.03} & \multirow{2}{*}{14.25} & \multirow{2}{*}{13.82} & \multirow{2}{*}{13.57} & \multirow{2}{*}{13.15} \\
			\multicolumn{2}{c}{latency (ms)}
			& & & & &
			\\
			\multicolumn{7}{c}{\vspace*{-3mm}}\\
			\multicolumn{2}{c}{Avg. synaptic}
			& \multirow{2}{*}{$83,691.48$}
			& \multirow{2}{*}{$835,274.98$}
			& \multirow{2}{*}{$4,173,392.03$}
			& \multirow{2}{*}{$8,343,559.69$}
			& \multirow{2}{*}{$83,385,785.67$}
			\\	
			\multicolumn{2}{c}{events (Sopbs)}
			& & & & &
			\\
			\multicolumn{7}{c}{\vspace*{-3mm}}\\
			Accuracy
			& N 
			& 79.63$\pm0.23$
			& 91.42$\pm0.13$ 
			& 92.99$\pm0.15$
			& 87.05$\pm0.21$
			& 89.63$\pm0.08$
			\\
			(\%)
			& S
			& 79.57$\pm0.31$
			& 91.39$\pm0.09$
			& 92.99$\pm0.08$
			& 87.00$\pm0.26$
			& 89.58$\pm0.24$
			\\
			& &\multicolumn{5}{c}{\vspace*{-4mm}}\\
			Avg. SIM
			& N 
			& 445.09
			& 503.21
			& 767.67
			& $1,131.09$
			& $12,027.75$
			\\
			time (s)
			& S
			& \protect\TLSdel{\multicolumn{5}{c}{$12,000$}} \protect\TLSins{\multicolumn{5}{c}{12K}}
			\\
			& &\multicolumn{5}{c}{\vspace*{-4mm}}\\
			Power
			& N 
			& 20
			& 20
			& 20
			& 19
			& 17
			\\
			(W) & S
			& 0.38 
			& 0.38 
			& 0.41
			& 0.44
			& 1.50
			\\
			& &\multicolumn{5}{c}{\vspace*{-4mm}}\\
			Energy
			& N 
			& 8.90
			& 10.06
			& 15.34
			& 21.50
			& 208.25
			\\
			(KJ) & S
			& 4.56
			& 4.56
			& 4.92
			& 5.28
			& 18.00
			\\
		\end{tabular}
		\egroup
		\label{tbl:compare}
	\end{center}
\end{sidewaystable}

As the network size grows there are more decision neurons and synapses connecting to them, thus the simulation time on NEST increases.
On the other hand, SpiNNaker works in (biologically) real time and the simulation time becomes shorter than the NEST simulation when \protect\TLSdel{$1,000$} \protect\TLSins{1K} patterns per digit \protect\TLSdel{($1,000$} \protect\TLSins{(1K} decision neurons per digit) are used.
The NEST simulation was run on a desktop PC, and the power use was measured by a meter socket and estimated by subtracting the usage of idle OS operation from the usage running the simulation.
In doing so, the power consumption of the resources needed to run the simulation is better approximated.
The SpiNNaker test was run on a Spin4 board which has 48 chips and exposed pins to measure electrical quantities.
A built-in Arduino board provided a measurement read out of the power usage of the chips.
For the same goal of estimating just the required resources, only the active chips were measured.
Even with the smallest network, SpiNNaker wins in the energy cost comparison, see Figure~\ref{fig:energy}.
Among different network configurations, the model with 500 decision neurons (50 clusters per digit) reaches the highest recognition rate of 92.99\% on average having a latency of 13.82~ms mean and 2.96~ms standard deviation.
And there are standard deviations of 2.57\% on CA and of 1.17~ms on the latency over 10 testing digits.
The total number of synaptic events is around $4.17$M Sopbs, where only 7K spikes are generated in the output layer. 
The NEST simulation costs 767.67~s on average for the entire \protect\TLSdel{$12,000$~s} \protect\TLSins{12K~s} biological-time test, 20~W in power use on the PC and $15.35$~KJ of energy, while SpiNNaker works in real time using $4.92$~KJ of energy at a power of 0.41~W (see Table~\ref{tbl:compare}).
This result provides a baseline for comparison with other SNN models and neuromorphic \protect\TLSdel{hardwares,} \protect\TLSins{hardware,} and no optimisation is applied.


\begin{figure}[hbt!]
	\centering
	\includegraphics[width=0.6\textwidth]{pics_bench/fig8.jpg}
	\caption{Energy usages of different network size both using NEST (blue) on a PC and SpiNNaker (black).}
	\label{fig:energy}
\end{figure}

\section{Summary}
\label{sec:summ}
This chapter puts forward the NE dataset as a baseline for comparisons of vision based SNN models and neuromorphic platforms.
It contains spike-based versions of existing widely-used databases in the vision recognition field.
Since new problems will continue to arise before vision becomes a solved question, the dataset will evolve as research progresses. 
The conversion methods for transforming images and videos into spike trains will advance. The number of vision datasets will increase and the corresponding evaluation methodologies will evolve.
The dataset aims to provide unified spike-based vision benchmarks and complementary evaluation methodologies to assess the performance of SNN algorithms.

